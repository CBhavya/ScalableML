{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Drabas & Lee  -- Learning PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the kernel to python 2!\n",
    "# you only need to run this cell if the spark context is not available when you start the notebook\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to create an RDD in PySpark. 1) You can parallelize a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(\n",
    "    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), \n",
    "     ('Amber', 9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or 2) read from a repository (a file or a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Projects/PySparkDemos\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_file = sc.textFile('data/VS14MORT.txt.gz', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/VS14MORT.txt.gz MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    }
   ],
   "source": [
    "print(data_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last parameter in sc.textFile(..., n) specifies the number of partitions the dataset is divided into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that to execute the code above you will have to change the path where the data is stored. The dataset can be downloaded from http://tomdrabas.com/data/VS14MORT.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can read from a multitude of filesystems: Local ones such as NTFS, FAT, or Mac OS Extended (HFS+), or distributed filesystems such as HDFS, S3, Cassandra, among many others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Spark can automatically work with compressed datasets (like the Gzipped one in our preceding example).\n",
    "\n",
    "Depending on how the data is read, the object holding it will be represented slightly  differently. The data read from a file is represented as MapPartitionsRDD instead  of ParallelCollectionRDD when we .paralellize(...) a collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are schema-less data structures (unlike DataFrames, which we will discuss later). Thus, parallelizing a dataset, such as in the following code snippet, \n",
    "is perfectly fine with Spark when using RDDs. We can mix amost everything: tuple, dict, list, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:475\n"
     ]
    }
   ],
   "source": [
    "data_heterogenous = sc.parallelize([('Ferrari', 'fast'), {'Porsche': 100000}, ['Spain','visited', 4504]])\n",
    "print(data_heterogenous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ferrari', 'fast'), {'Porsche': 100000}, ['Spain', 'visited', 4504]]\n"
     ]
    }
   ],
   "source": [
    "data_heterogenous_collected = data_heterogenous.collect() \n",
    "print(data_heterogenous_collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .collect() method returns all the elements of the RDD to the driver where it is serialized as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the data in the object as you would normally do in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heterogenous_collected[1]['Porsche']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spain', 'visited', 4504]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heterogenous_collected[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ferrari', 'fast')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heterogenous_collected[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you read from a text file, each row from the file forms an element of an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 8, localhost): org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\n      mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n    File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 109, in _get_module_details\n      __import__(pkg_name)\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\", line 44, in <module>\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 33, in <module>\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/java_gateway.py\", line 25, in <module>\n    File \"/home/ubuntu/anaconda3/lib/python3.6/platform.py\", line 886, in <module>\n      \"system node release version machine processor\")\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 381, in namedtuple\n  TypeError: namedtuple() missing 3 required keyword-only arguments: 'verbose', 'rename', and 'module'\nPYTHONPATH was:\n  /home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/jars/spark-core_2.11-2.0.0.jar:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python:/home/ubuntu/src/cntk/bindings/python\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:166)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:114)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\n      mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n    File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 109, in _get_module_details\n      __import__(pkg_name)\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\", line 44, in <module>\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 33, in <module>\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/java_gateway.py\", line 25, in <module>\n    File \"/home/ubuntu/anaconda3/lib/python3.6/platform.py\", line 886, in <module>\n      \"system node release version machine processor\")\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 381, in namedtuple\n  TypeError: namedtuple() missing 3 required keyword-only arguments: 'verbose', 'rename', and 'module'\nPYTHONPATH was:\n  /home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/jars/spark-core_2.11-2.0.0.jar:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python:/home/ubuntu/src/cntk/bindings/python\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:166)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:114)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0329003b6562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_from_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 8, localhost): org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\n      mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n    File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 109, in _get_module_details\n      __import__(pkg_name)\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\", line 44, in <module>\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 33, in <module>\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/java_gateway.py\", line 25, in <module>\n    File \"/home/ubuntu/anaconda3/lib/python3.6/platform.py\", line 886, in <module>\n      \"system node release version machine processor\")\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 381, in namedtuple\n  TypeError: namedtuple() missing 3 required keyword-only arguments: 'verbose', 'rename', and 'module'\nPYTHONPATH was:\n  /home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/jars/spark-core_2.11-2.0.0.jar:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python:/home/ubuntu/src/cntk/bindings/python\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:166)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:114)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\n      mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n    File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 109, in _get_module_details\n      __import__(pkg_name)\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\", line 44, in <module>\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 33, in <module>\n    File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n    File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 656, in _load_unlocked\n    File \"<frozen importlib._bootstrap>\", line 626, in _load_backward_compatible\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/java_gateway.py\", line 25, in <module>\n    File \"/home/ubuntu/anaconda3/lib/python3.6/platform.py\", line 886, in <module>\n      \"system node release version machine processor\")\n    File \"/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 381, in namedtuple\n  TypeError: namedtuple() missing 3 required keyword-only arguments: 'verbose', 'rename', and 'module'\nPYTHONPATH was:\n  /home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/jars/spark-core_2.11-2.0.0.jar:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python/:/home/ubuntu/spark/spark-2.0.0-bin-hadoop2.7/python:/home/ubuntu/src/cntk/bindings/python\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:166)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:114)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "data_from_file.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'                   1                                          2101  M1087 432311  4M4                2014U7CN                                    I64 238 070   24 0111I64                                                                                                                                                                           01 I64                                                                                                  01  11                                 100 601',\n",
       " u'                   1                                          2101  M1058 371708  4D3                2014U7CN                                    I250214 062   21 0311I250 61I272 62E669                                                                                                                                                            03 I250 E669 I272                                                                                       01  11                                 100 601']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create *longer* method to transform your data instead of using the `lambda` expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractInformation(row):\n",
    "    import re\n",
    "    import numpy as np\n",
    "\n",
    "    selected_indices = [\n",
    "         2,4,5,6,7,9,10,11,12,13,14,15,16,17,18,\n",
    "         19,21,22,23,24,25,27,28,29,30,32,33,34,\n",
    "         36,37,38,39,40,41,42,43,44,45,46,47,48,\n",
    "         49,50,51,52,53,54,55,56,58,60,61,62,63,\n",
    "         64,65,66,67,68,69,70,71,72,73,74,75,76,\n",
    "         77,78,79,81,82,83,84,85,87,89\n",
    "    ]\n",
    "\n",
    "    '''\n",
    "        Input record schema\n",
    "        schema: n-m (o) -- xxx\n",
    "            n - position from\n",
    "            m - position to\n",
    "            o - number of characters\n",
    "            xxx - description\n",
    "        1. 1-19 (19) -- reserved positions\n",
    "        2. 20 (1) -- resident status\n",
    "        3. 21-60 (40) -- reserved positions\n",
    "        4. 61-62 (2) -- education code (1989 revision)\n",
    "        5. 63 (1) -- education code (2003 revision)\n",
    "        6. 64 (1) -- education reporting flag\n",
    "        7. 65-66 (2) -- month of death\n",
    "        8. 67-68 (2) -- reserved positions\n",
    "        9. 69 (1) -- sex\n",
    "        10. 70 (1) -- age: 1-years, 2-months, 4-days, 5-hours, 6-minutes, 9-not stated\n",
    "        11. 71-73 (3) -- number of units (years, months etc)\n",
    "        12. 74 (1) -- age substitution flag (if the age reported in positions 70-74 is calculated using dates of birth and death)\n",
    "        13. 75-76 (2) -- age recoded into 52 categories\n",
    "        14. 77-78 (2) -- age recoded into 27 categories\n",
    "        15. 79-80 (2) -- age recoded into 12 categories\n",
    "        16. 81-82 (2) -- infant age recoded into 22 categories\n",
    "        17. 83 (1) -- place of death\n",
    "        18. 84 (1) -- marital status\n",
    "        19. 85 (1) -- day of the week of death\n",
    "        20. 86-101 (16) -- reserved positions\n",
    "        21. 102-105 (4) -- current year\n",
    "        22. 106 (1) -- injury at work\n",
    "        23. 107 (1) -- manner of death\n",
    "        24. 108 (1) -- manner of disposition\n",
    "        25. 109 (1) -- autopsy\n",
    "        26. 110-143 (34) -- reserved positions\n",
    "        27. 144 (1) -- activity code\n",
    "        28. 145 (1) -- place of injury\n",
    "        29. 146-149 (4) -- ICD code\n",
    "        30. 150-152 (3) -- 358 cause recode\n",
    "        31. 153 (1) -- reserved position\n",
    "        32. 154-156 (3) -- 113 cause recode\n",
    "        33. 157-159 (3) -- 130 infant cause recode\n",
    "        34. 160-161 (2) -- 39 cause recode\n",
    "        35. 162 (1) -- reserved position\n",
    "        36. 163-164 (2) -- number of entity-axis conditions\n",
    "        37-56. 165-304 (140) -- list of up to 20 conditions\n",
    "        57. 305-340 (36) -- reserved positions\n",
    "        58. 341-342 (2) -- number of record axis conditions\n",
    "        59. 343 (1) -- reserved position\n",
    "        60-79. 344-443 (100) -- record axis conditions\n",
    "        80. 444 (1) -- reserve position\n",
    "        81. 445-446 (2) -- race\n",
    "        82. 447 (1) -- bridged race flag\n",
    "        83. 448 (1) -- race imputation flag\n",
    "        84. 449 (1) -- race recode (3 categories)\n",
    "        85. 450 (1) -- race recode (5 categories)\n",
    "        86. 461-483 (33) -- reserved positions\n",
    "        87. 484-486 (3) -- Hispanic origin\n",
    "        88. 487 (1) -- reserved\n",
    "        89. 488 (1) -- Hispanic origin/race recode\n",
    "     '''\n",
    "\n",
    "    record_split = re\\\n",
    "        .compile(\n",
    "            r'([\\s]{19})([0-9]{1})([\\s]{40})([0-9\\s]{2})([0-9\\s]{1})([0-9]{1})([0-9]{2})' + \n",
    "            r'([\\s]{2})([FM]{1})([0-9]{1})([0-9]{3})([0-9\\s]{1})([0-9]{2})([0-9]{2})' + \n",
    "            r'([0-9]{2})([0-9\\s]{2})([0-9]{1})([SMWDU]{1})([0-9]{1})([\\s]{16})([0-9]{4})' +\n",
    "            r'([YNU]{1})([0-9\\s]{1})([BCOU]{1})([YNU]{1})([\\s]{34})([0-9\\s]{1})([0-9\\s]{1})' +\n",
    "            r'([A-Z0-9\\s]{4})([0-9]{3})([\\s]{1})([0-9\\s]{3})([0-9\\s]{3})([0-9\\s]{2})([\\s]{1})' + \n",
    "            r'([0-9\\s]{2})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([\\s]{36})([A-Z0-9\\s]{2})([\\s]{1})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([\\s]{1})([0-9\\s]{2})([0-9\\s]{1})' + \n",
    "            r'([0-9\\s]{1})([0-9\\s]{1})([0-9\\s]{1})([\\s]{33})([0-9\\s]{3})([0-9\\s]{1})([0-9\\s]{1})')\n",
    "    \n",
    "    #parsing starts. When parsing fails we just put a -99 there to indicated parsing failed in that row. \n",
    "    try:\n",
    "        rs = np.array(record_split.split(row))[selected_indices]\n",
    "    except:\n",
    "        rs = np.array(['-99'] * len(selected_indices))\n",
    "    return rs\n",
    "#     return record_split.split(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Defining pure python methods can slow down your application because Spark constantly needs to switch between Python interpreter and JVM. Whenver possible, we should you built-in python functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of using `lambda` we will use the `extractInformation(...)` method to split and convert our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_file_conv = data_from_file.map(extractInformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'1', u'  ', u'2', u'1', u'01', u'M', u'1', u'087', u' ', u'43',\n",
       "        u'23', u'11', u'  ', u'4', u'M', u'4', u'2014', u'U', u'7', u'C',\n",
       "        u'N', u' ', u' ', u'I64 ', u'238', u'070', u'   ', u'24', u'01',\n",
       "        u'11I64  ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'01',\n",
       "        u'I64  ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file_conv.map(lambda row: row).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .map(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is applied to each element of the RDD: in the case for the `data_from_file_conv` dataset you can think of this as a transformation of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, -99]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2014 = data_from_file_conv.map(lambda row: int(row[16]))\n",
    "data_2014.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2014', 2014),\n",
       " (u'2014', 2014),\n",
       " (u'2014', 2014),\n",
       " (u'2014', 2014),\n",
       " (u'2014', 2014),\n",
       " (u'2014', 2014),\n",
       " (u'2014', 2014),\n",
       " (u'2014', 2014),\n",
       " (u'2014', 2014),\n",
       " ('-99', -99)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2014_2 = data_from_file_conv.map(lambda row: (row[16], int(row[16])))\n",
    "data_2014_2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .filter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.filter(...)` method allows you to select elements of your dataset that fit specified criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_from_file_conv.filter(lambda row: row[5] == 'F' and row[21] == '0')\n",
    "data_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'2', u'12', u' ', u'0', u'07', u'F', u'1', u'030', u' ', u'32',\n",
       "        u'12', u'05', u'  ', u'1', u'D', u'6', u'2014', u'N', u'1', u'U',\n",
       "        u'Y', u'0', u'9', u'X44 ', u'420', u'122', u'   ', u'39', u'05',\n",
       "        u'11T391 ', u'12X44  ', u'13T401 ', u'14T424 ', u'61F199 ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'05',\n",
       "        u'X44  ', u'F199 ', u'T391 ', u'T401 ', u'T424 ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40'),\n",
       " array([u'1', u'  ', u'5', u'1', u'08', u'F', u'1', u'051', u' ', u'36',\n",
       "        u'16', u'07', u'  ', u'4', u'D', u'4', u'2014', u'N', u'7', u'C',\n",
       "        u'N', u'0', u'0', u'C159', u'077', u'021', u'   ', u'15', u'04',\n",
       "        u'11C159 ', u'61T149 ', u'62W18  ', u'63F179 ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'04',\n",
       "        u'C159 ', u'F179 ', u'T149 ', u'W18  ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'02', u' ', u' ', u'3', u'2', u'100', u'7'],\n",
       "       dtype='<U40'),\n",
       " array([u'1', u'  ', u'6', u'1', u'08', u'F', u'1', u'054', u' ', u'36',\n",
       "        u'16', u'07', u'  ', u'7', u'M', u'6', u'2014', u'N', u'1', u'C',\n",
       "        u'Y', u'0', u' ', u'V139', u'387', u'114', u'   ', u'38', u'03',\n",
       "        u'11T07  ', u'12V139 ', u'13S029 ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'03',\n",
       "        u'V139 ', u'S029 ', u'T07  ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40'),\n",
       " array([u'1', u'  ', u'1', u'1', u'08', u'F', u'1', u'006', u' ', u'27',\n",
       "        u'07', u'03', u'  ', u'3', u'S', u'3', u'2014', u'N', u'1', u'B',\n",
       "        u'Y', u'0', u' ', u'V139', u'387', u'114', u'   ', u'38', u'03',\n",
       "        u'11S099 ', u'12V139 ', u'13S069 ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'03',\n",
       "        u'V139 ', u'S069 ', u'S099 ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'03', u' ', u' ', u'2', u'3', u'100', u'8'],\n",
       "       dtype='<U40'),\n",
       " array([u'2', u'  ', u'4', u'1', u'08', u'F', u'1', u'052', u' ', u'36',\n",
       "        u'16', u'07', u'  ', u'7', u'M', u'1', u'2014', u'N', u'1', u'C',\n",
       "        u'N', u'0', u' ', u'V139', u'387', u'114', u'   ', u'38', u'03',\n",
       "        u'11S099 ', u'12V139 ', u'13S199 ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'03',\n",
       "        u'V139 ', u'S099 ', u'S199 ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40'),\n",
       " array([u'2', u'  ', u'7', u'1', u'11', u'F', u'1', u'025', u' ', u'31',\n",
       "        u'11', u'05', u'  ', u'1', u'S', u'5', u'2014', u'N', u'1', u'C',\n",
       "        u'N', u'0', u' ', u'V031', u'386', u'114', u'   ', u'38', u'02',\n",
       "        u'11S099 ', u'12V031 ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'02',\n",
       "        u'V031 ', u'S099 ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .flatMap(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.flatMap(...)` method works similarly to `.map(...)` but returns a flattened results instead of a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered_flat = data_filtered.flatMap(lambda row: (row[16], int(row[16]) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered_flat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2014',\n",
       " 2015,\n",
       " u'2014',\n",
       " 2015,\n",
       " u'2014',\n",
       " 2015,\n",
       " u'2014',\n",
       " 2015,\n",
       " u'2014',\n",
       " 2015,\n",
       " u'2014',\n",
       " 2015]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered_flat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2014', 2015, u'2014', 2015, u'2014', 2015, u'2014', 2015, u'2014', 2015]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2014_flat = data_from_file_conv.flatMap(lambda row: (row[16], int(row[16]) + 1))\n",
    "data_2014_flat.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5262342"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2014_flat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a list of distinct values in a specified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-99', u'M', u'F']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_gender = data_from_file_conv.map(lambda row: row[5]).distinct().collect()\n",
    "distinct_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .sample(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.sample()` method returns a randomized sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'1', u'  ', u'3', u'1', u'01', u'F', u'1', u'045', u' ', u'35',\n",
       "        u'15', u'07', u'  ', u'4', u'M', u'6', u'2014', u'U', u'7', u'C',\n",
       "        u'N', u' ', u' ', u'I48 ', u'228', u'068', u'   ', u'22', u'01',\n",
       "        u'11I48  ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'01',\n",
       "        u'I48  ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction = 0.01\n",
    "#False, fraction, 666 = With raplecement? Fraction of dataset used to sampling, random seed\n",
    "data_sample = data_from_file_conv.sample(False, fraction, 666)\n",
    "\n",
    "data_sample.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that we really got 1% of all the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 2631171, sample: 26242\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset: {0}, sample: {1}'.format(data_from_file_conv.count(), data_sample.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .leftOuterJoin(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left outer join, just like the SQL world, joins two RDDs based on the values found in both datasets, and returns records from the left RDD with records from the right one appended where the two RDDs match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 4)), ('a', (1, 1)), ('c', (10, None)), ('b', (4, '6'))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a', 1), ('b', 4), ('c',10)])\n",
    "rdd2 = sc.parallelize([('a', 4), ('a', 1), ('b', '6'), ('d', 15)])\n",
    "\n",
    "rdd3 = rdd1.leftOuterJoin(rdd2)\n",
    "rdd3.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d is missing since this is only a leftOuterJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used `.join(...)` method instead we would have gotten only the values for `'a'` and `'b'` as these two values intersect between these two RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 4)), ('a', (1, 1)), ('b', (4, '6'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = rdd1.join(rdd2)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is the `.intersection(...)` that returns the records that are *equal* in both RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = rdd1.intersection(rdd2)\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .repartition(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartitioning the dataset changes the number of partitions the dataset is divided into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd1.repartition(4)\n",
    "\n",
    "len(rdd1.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [('a', 1), ('b', 4), ('c', 10)]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .take(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method returns `n` top rows from a single data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'1', u'  ', u'2', u'1', u'01', u'M', u'1', u'087', u' ', u'43',\n",
       "        u'23', u'11', u'  ', u'4', u'M', u'4', u'2014', u'U', u'7', u'C',\n",
       "        u'N', u' ', u' ', u'I64 ', u'238', u'070', u'   ', u'24', u'01',\n",
       "        u'11I64  ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'01',\n",
       "        u'I64  ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_first = data_from_file_conv.take(1)\n",
    "data_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want somewhat randomized records you can use `.takeSample(...)` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'1', u'  ', u'3', u'1', u'01', u'F', u'1', u'085', u' ', u'43',\n",
       "        u'23', u'11', u'  ', u'4', u'D', u'6', u'2014', u'U', u'7', u'B',\n",
       "        u'N', u' ', u' ', u'I251', u'215', u'063', u'   ', u'21', u'05',\n",
       "        u'11I259 ', u'21I251 ', u'61I500 ', u'62E149 ', u'63F179 ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'04',\n",
       "        u'I251 ', u'E149 ', u'F179 ', u'I500 ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40'),\n",
       " array([u'1', u'  ', u'3', u'1', u'10', u'F', u'1', u'093', u' ', u'44',\n",
       "        u'24', u'11', u'  ', u'4', u'W', u'7', u'2014', u'U', u'7', u'B',\n",
       "        u'N', u' ', u' ', u'E875', u'172', u'111', u'   ', u'37', u'03',\n",
       "        u'11I469 ', u'21E875 ', u'61C80  ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'03',\n",
       "        u'E875 ', u'C80  ', u'I469 ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'],\n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_take_sampled = data_from_file_conv.takeSample(False, 2, 667)\n",
    "data_take_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .reduce(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another action that processes your data, the `.reduce(...)` method *reduces* the elements of an RDD using a specified method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 4), ('c', 10)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda row: row[1]).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the reducing function is not associative and commutative you will sometimes get wrong results depending how your data is partitioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda row: row[1]).reduce(lambda x, y: x / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduce = sc.parallelize([1.0, 2.0, .5, .1, 5, .2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 0.5, 0.1, 5, 0.2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduce.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to reduce the data in a manner that we would like to *divide* the current result by the subsequent one, we would expect a value of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "works = data_reduce.reduce(lambda x, y: x / y)\n",
    "works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you were to partition the data into 3 partitions, the result will be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduce = sc.parallelize([1.0, 2.0, .5, .1, 5, .2], 3)\n",
    "data_reduce.reduce(lambda x, y: x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.reduceByKey(...)` method works in a similar way to the `.reduce(...)` method but performs a reduction on a key-by-key basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 12), ('d', 5), ('c', 2), ('b', 4)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key = sc.parallelize([('a', 4),('b', 3),('c', 2),('a', 8),('d', 2),('b', 1),('d', 3)],4)\n",
    "data_key.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.count()` method counts the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduce.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the same effect as the method below but does not require shifting the data to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_reduce.collect()) # WRONG -- DON'T DO THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset is in a form of a *key-value* you can use the `.countByKey()` method to get the counts of distinct keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('c', 1), ('b', 2), ('d', 2)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.countByKey().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .saveAsTextFile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, the `.saveAsTextFile()` the RDD and saves it to text files: each partition to a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key.saveAsTextFile('data_key.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read it back, you need to parse it back as, as before, all the rows are treated as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'b', 3), (u'c', 2), (u'a', 4), (u'b', 1), (u'd', 3), (u'a', 8), (u'd', 2)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseInput(row):\n",
    "    import re\n",
    "    \n",
    "    pattern = re.compile(r'\\(\\'([a-z])\\', ([0-9])\\)')\n",
    "    row_split = pattern.split(row)\n",
    "    \n",
    "    return (row_split[1], int(row_split[2]))\n",
    "    \n",
    "data_key_reread = sc \\\n",
    "    .textFile('data_key.txt') \\\n",
    "    .map(parseInput)\n",
    "data_key_reread.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.foreach(...)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method that applies the same function to each element of the RDD in an iterative way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prints to terminal not to jupyter notebook!\n",
    "def f(x): \n",
    "    print(x)\n",
    "    print('hi')\n",
    "\n",
    "data_key.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 3), ('c', 2), ('a', 8), ('d', 2), ('b', 1), ('d', 3)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
