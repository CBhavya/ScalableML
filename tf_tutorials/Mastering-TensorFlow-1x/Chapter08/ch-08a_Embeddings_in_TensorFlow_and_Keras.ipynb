{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Embeddings-with-PTB-Data-in-TensorFlow\" data-toc-modified-id=\"Embeddings-with-PTB-Data-in-TensorFlow-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Embeddings with PTB Data in TensorFlow</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-and-Prepare-PTB-data\" data-toc-modified-id=\"Load-and-Prepare-PTB-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load and Prepare PTB data</a></span></li><li><span><a href=\"#Train-the-skip-gram-model-for-PTB-Data-in-TensorFlow\" data-toc-modified-id=\"Train-the-skip-gram-model-for-PTB-Data-in-TensorFlow-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Train the skip-gram model for PTB Data in TensorFlow</a></span></li></ul></li><li><span><a href=\"#Embeddings-with-Text8-data-in-TensorFlow\" data-toc-modified-id=\"Embeddings-with-Text8-data-in-TensorFlow-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Embeddings with Text8 data in TensorFlow</a></span></li><li><span><a href=\"#skip-gram-model-with-Keras\" data-toc-modified-id=\"skip-gram-model-with-Keras-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>skip-gram model with Keras</a></span></li><li><span><a href=\"#word2vec-or-embeddings-visualisation-using-TensorBoard\" data-toc-modified-id=\"word2vec-or-embeddings-visualisation-using-TensorBoard-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>word2vec or embeddings visualisation using TensorBoard</a></span></li><li><span><a href=\"#The-word2id-and-id2word-code-explained\" data-toc-modified-id=\"The-word2id-and-id2word-code-explained-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>The word2id and id2word code explained</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors and Embeddings in TensorFlow and Keras <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:1.14.5\n",
      "Matplotlib:2.2.2\n",
      "TensorFlow:1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(\"NumPy:{}\".format(np.__version__))\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize']=15,10\n",
    "print(\"Matplotlib:{}\".format(mpl.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "print(\"TensorFlow:{}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETSLIB_HOME = '../datasetslib'\n",
    "import sys\n",
    "if not DATASETSLIB_HOME in sys.path:\n",
    "    sys.path.append(DATASETSLIB_HOME)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import datasetslib\n",
    "\n",
    "from datasetslib import util as dsu\n",
    "from datasetslib import nputil\n",
    "\n",
    "datasetslib.datasets_root = os.path.join(os.path.expanduser('~'),'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings with PTB Data in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare PTB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: /home/ubuntu/datasets/ptb-simple/simple-examples.tgz\n",
      "Train : [9970 9971 9972 9974 9975]\n",
      "Test:  [102  14  24  32 752]\n",
      "Valid:  [1132   93  358    5  329]\n",
      "Vocabulary Length =  10000\n"
     ]
    }
   ],
   "source": [
    "from datasetslib.ptb import PTBSimple\n",
    "ptb = PTBSimple()\n",
    "# downloads data, converts words to ids, converts files to a list of ids\n",
    "ptb.load_data()\n",
    "print('Train :', ptb.part['train'][0:5])\n",
    "print('Test: ', ptb.part['test'][0:5])\n",
    "print('Valid: ', ptb.part['valid'][0:5])\n",
    "print('Vocabulary Length = ', ptb.vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CBOW pairs : context,target\n",
      "( ['aer', 'banknote', 'calloway', 'centrust'] , 9972 berlitz )\n",
      "( ['banknote', 'berlitz', 'centrust', 'cluett'] , 9974 calloway )\n",
      "( ['berlitz', 'calloway', 'cluett', 'fromstein'] , 9975 centrust )\n",
      "( ['calloway', 'centrust', 'fromstein', 'gitano'] , 9976 cluett )\n",
      "( ['centrust', 'cluett', 'gitano', 'guterman'] , 9980 fromstein )\n",
      "( ['cluett', 'fromstein', 'guterman', 'hydro-quebec'] , 9981 gitano )\n",
      "( ['fromstein', 'gitano', 'hydro-quebec', 'ipo'] , 9982 guterman )\n",
      "( ['gitano', 'guterman', 'ipo', 'kia'] , 9983 hydro-quebec )\n",
      "( ['guterman', 'hydro-quebec', 'kia', 'memotec'] , 9984 ipo )\n",
      "( ['hydro-quebec', 'ipo', 'memotec', 'mlx'] , 9986 kia )\n"
     ]
    }
   ],
   "source": [
    "ptb.skip_window = 2\n",
    "ptb.reset_index()\n",
    "# in CBOW input is the context word and output is the target word\n",
    "y_batch, x_batch = ptb.next_batch_cbow()\n",
    "\n",
    "print('The CBOW pairs : context,target')\n",
    "for i in range(5 * ptb.skip_window):\n",
    "    print('(', [ptb.id2word[x_i] for x_i in x_batch[i]],\n",
    "          ',', y_batch[i], ptb.id2word[y_batch[i]], ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skip-gram pairs : target,context\n",
      "( 9972 berlitz , 9970 aer )\n",
      "( 9972 berlitz , 9971 banknote )\n",
      "( 9972 berlitz , 9974 calloway )\n",
      "( 9972 berlitz , 9975 centrust )\n",
      "( 9974 calloway , 9971 banknote )\n",
      "( 9974 calloway , 9972 berlitz )\n",
      "( 9974 calloway , 9975 centrust )\n",
      "( 9974 calloway , 9976 cluett )\n",
      "( 9975 centrust , 9972 berlitz )\n",
      "( 9975 centrust , 9974 calloway )\n"
     ]
    }
   ],
   "source": [
    "ptb.skip_window = 2\n",
    "ptb.reset_index()\n",
    "# in skip-gram input is the target word and output is the context word\n",
    "x_batch, y_batch = ptb.next_batch_sg()\n",
    "\n",
    "print('The skip-gram pairs : target,context')\n",
    "for i in range(5 * ptb.skip_window):\n",
    "    print('(', x_batch[i], ptb.id2word[x_batch[i]],\n",
    "          ',', y_batch[i], ptb.id2word[y_batch[i]], ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  [64 58 59  4 69 53 31 77]\n"
     ]
    }
   ],
   "source": [
    "valid_size = 8\n",
    "x_valid = np.random.choice(valid_size * 10, valid_size, replace=False)\n",
    "print('valid: ',x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the skip-gram model for PTB Data in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  \n",
    "n_negative_samples = 64\n",
    "ptb.skip_window=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the effects of previous sessions in the Jupyter Notebook\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(dtype=tf.int32, shape=[batch_size])\n",
    "outputs = tf.placeholder(dtype=tf.int32, shape=[batch_size,1])\n",
    "inputs_valid = tf.constant(x_valid, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embeddings matrix with vocab_len rows and embedding_size columns\n",
    "# each row represents vectore representation or embedding of a word\n",
    "# in the vocbulary\n",
    "\n",
    "embed_dist = tf.random_uniform(shape=[ptb.vocab_len, embedding_size],\n",
    "                               minval=-1.0,\n",
    "                               maxval=1.0\n",
    "                               )\n",
    "embed_matrix = tf.Variable(embed_dist,\n",
    "                           name='embed_matrix'\n",
    "                           )\n",
    "# define the embedding lookup table\n",
    "# provides the embeddings of the word ids in the input tensor\n",
    "embed_ltable = tf.nn.embedding_lookup(embed_matrix, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define noise-contrastive estimation (NCE) loss layer\n",
    "\n",
    "nce_dist = tf.truncated_normal(shape=[ptb.vocab_len, embedding_size],\n",
    "                               stddev=1.0 /\n",
    "                               tf.sqrt(embedding_size * 1.0)\n",
    "                               )\n",
    "nce_w = tf.Variable(nce_dist)\n",
    "nce_b = tf.Variable(tf.zeros(shape=[ptb.vocab_len]))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_w,\n",
    "                                     biases=nce_b,\n",
    "                                     inputs=embed_ltable,\n",
    "                                     labels=outputs,\n",
    "                                     num_sampled=n_negative_samples,\n",
    "                                     num_classes=ptb.vocab_len\n",
    "                                     )\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-fa352dc11208>:4: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Compute the cosine similarity between validation set samples\n",
    "# and all embeddings.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embed_matrix), 1, \n",
    "                             keep_dims=True))\n",
    "normalized_embeddings = embed_matrix / norm\n",
    "embed_valid = tf.nn.embedding_lookup(normalized_embeddings, \n",
    "                                     inputs_valid)\n",
    "similarity = tf.matmul(\n",
    "    embed_valid, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss after epoch  0 :  115.84483724985367\n",
      "Similar to we: free, launched, hilton, than, better,\n",
      "Similar to been: of, mortgage, trade, expenses, chief,\n",
      "Similar to also: <unk>, was, read, less, markets,\n",
      "Similar to of: said, been, it, at, there,\n",
      "Similar to last: on, two, act, with, changed,\n",
      "Similar to u.s.: 's, N, b., million, down,\n",
      "Similar to an: early, any, than, way, inventories,\n",
      "Similar to trading: at, spending, of, substantially, reporting,\n",
      "\n",
      "Average loss after epoch  1 :  49.465019134374764\n",
      "Similar to we: free, launched, better, ministry, seems,\n",
      "Similar to been: of, magazine, mortgage, expenses, materials,\n",
      "Similar to also: read, <unk>, advertisers, less, moscow,\n",
      "Similar to of: said, been, earlier, stevens, there,\n",
      "Similar to last: on, changed, brief, two, act,\n",
      "Similar to u.s.: b., 's, down, treasury, N,\n",
      "Similar to an: any, early, way, devices, workers,\n",
      "Similar to trading: spending, at, explained, reporting, declaration,\n",
      "\n",
      "Average loss after epoch  2 :  28.344913572837147\n",
      "Similar to we: free, launched, seems, ministry, better,\n",
      "Similar to been: forum, of, magazine, materials, mortgage,\n",
      "Similar to also: <unk>, read, less, nor, p.,\n",
      "Similar to of: stevens, been, said, <eos>, if,\n",
      "Similar to last: on, brief, changed, specialists, act,\n",
      "Similar to u.s.: b., 's, treasury, down, enact,\n",
      "Similar to an: any, early, devices, way, led,\n",
      "Similar to trading: explained, label, reporting, declaration, at,\n",
      "\n",
      "Average loss after epoch  3 :  18.6763108418538\n",
      "Similar to we: free, launched, seems, ministry, better,\n",
      "Similar to been: forum, materials, magazine, mortgage, of,\n",
      "Similar to also: read, nor, investor, <unk>, exhibition,\n",
      "Similar to of: deaths, stevens, place, settle, found,\n",
      "Similar to last: brief, on, changed, specialists, act,\n",
      "Similar to u.s.: b., 's, enact, treasury, down,\n",
      "Similar to an: devices, why, smooth, banking, led,\n",
      "Similar to trading: explained, label, reporting, declaration, attached,\n",
      "\n",
      "Average loss after epoch  4 :  13.75675258346093\n",
      "Similar to we: free, launched, seems, ministry, better,\n",
      "Similar to been: forum, materials, magazine, of, mortgage,\n",
      "Similar to also: <unk>, read, investor, nor, exhibition,\n",
      "Similar to of: deaths, stevens, place, particular, <eos>,\n",
      "Similar to last: brief, changed, on, specialists, via,\n",
      "Similar to u.s.: b., enact, treasury, 's, schools,\n",
      "Similar to an: devices, smooth, why, banking, led,\n",
      "Similar to trading: explained, label, reporting, declaration, attached,\n",
      "\n",
      "Average loss after epoch  5 :  10.141242457505983\n",
      "Similar to we: free, launched, seems, ministry, better,\n",
      "Similar to been: forum, materials, magazine, mortgage, of,\n",
      "Similar to also: <unk>, investor, read, exhibition, nor,\n",
      "Similar to of: stevens, deaths, particular, place, found,\n",
      "Similar to last: brief, changed, on, specialists, via,\n",
      "Similar to u.s.: b., enact, 's, schools, treasury,\n",
      "Similar to an: smooth, why, devices, banking, led,\n",
      "Similar to trading: explained, label, reporting, attached, declaration,\n",
      "\n",
      "Average loss after epoch  6 :  8.608646925443258\n",
      "Similar to we: free, launched, seems, ministry, better,\n",
      "Similar to been: forum, materials, magazine, stars, mortgage,\n",
      "Similar to also: <unk>, exhibition, read, investor, keep,\n",
      "Similar to of: stevens, deaths, particular, place, valuation,\n",
      "Similar to last: brief, changed, specialists, via, act,\n",
      "Similar to u.s.: b., enact, 's, treasury, schools,\n",
      "Similar to an: smooth, why, devices, banking, led,\n",
      "Similar to trading: explained, label, reporting, attached, declaration,\n",
      "\n",
      "Average loss after epoch  7 :  6.938401744151727\n",
      "Similar to we: free, seems, launched, ministry, better,\n",
      "Similar to been: forum, materials, magazine, stars, of,\n",
      "Similar to also: <unk>, investor, exhibition, read, nor,\n",
      "Similar to of: stevens, deaths, place, particular, valuation,\n",
      "Similar to last: changed, brief, via, specialists, act,\n",
      "Similar to u.s.: b., enact, 's, treasury, monthly,\n",
      "Similar to an: smooth, banking, why, devices, arm,\n",
      "Similar to trading: explained, label, attached, reporting, declaration,\n",
      "\n",
      "Average loss after epoch  8 :  6.227316045608276\n",
      "Similar to we: free, seems, launched, ministry, better,\n",
      "Similar to been: forum, materials, of, magazine, stars,\n",
      "Similar to also: <unk>, investor, exhibition, read, nightmare,\n",
      "Similar to of: deaths, stevens, particular, place, associates,\n",
      "Similar to last: changed, brief, via, specialists, firm,\n",
      "Similar to u.s.: b., enact, treasury, includes, monthly,\n",
      "Similar to an: smooth, banking, why, devices, arm,\n",
      "Similar to trading: explained, label, attached, reporting, developing,\n",
      "\n",
      "Average loss after epoch  9 :  5.525094214922342\n",
      "Similar to we: free, seems, launched, ministry, better,\n",
      "Similar to been: forum, materials, magazine, of, cigarette,\n",
      "Similar to also: <unk>, exhibition, read, investor, nightmare,\n",
      "Similar to of: deaths, stevens, particular, associates, place,\n",
      "Similar to last: changed, brief, via, firm, specialists,\n",
      "Similar to u.s.: b., enact, monthly, includes, surviving,\n",
      "Similar to an: smooth, banking, why, devices, seemed,\n",
      "Similar to trading: explained, label, attached, reporting, developing,\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "learning_rate = 0.9\n",
    "n_batches = ptb.n_batches_wv()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        ptb.reset_index()\n",
    "        for step in range(n_batches):\n",
    "            x_batch, y_batch = ptb.next_batch_sg()\n",
    "            y_batch = nputil.to2d(y_batch, unit_axis=1)\n",
    "            feed_dict = {inputs: x_batch, outputs: y_batch}\n",
    "            _, batch_loss = tfs.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            epoch_loss += batch_loss\n",
    "        epoch_loss = epoch_loss / n_batches\n",
    "        print('\\nAverage loss after epoch ', epoch, ': ', epoch_loss)\n",
    "\n",
    "        # print closest words to validation set at end of every epoch\n",
    "        similarity_scores = tfs.run(similarity)\n",
    "        top_k = 5\n",
    "        for i in range(valid_size):\n",
    "            similar_words = (-similarity_scores[i, :]\n",
    "                             ).argsort()[1:top_k + 1]\n",
    "            similar_str = 'Similar to {0:}:'.format(\n",
    "                ptb.id2word[x_valid[i]])\n",
    "            for k in range(top_k):\n",
    "                similar_str = '{0:} {1:},'.format(\n",
    "                    similar_str, ptb.id2word[similar_words[k]])\n",
    "            print(similar_str)\n",
    "    final_embeddings = tfs.run(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "learning_rate = 0.9\n",
    "\n",
    "n_batches = ptb.n_batches_wv()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tfs.run(tf.global_variables_initializer())\n",
    "    epoch_loss = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        ptb.reset_index()\n",
    "        for step in range(n_batches):\n",
    "            x_batch, y_batch = ptb.next_batch_sg()\n",
    "            y_batch = nputil.to2d(y_batch, unit_axis=1)\n",
    "            feed_dict = {inputs: x_batch, outputs: y_batch}\n",
    "            _, batch_loss = tfs.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            epoch_loss += batch_loss\n",
    "        epoch_loss = epoch_loss / n_batches\n",
    "        if epoch + 1 % 1000 == 0:\n",
    "            print('epoch done: ', epoch)\n",
    "    print('\\nAverage loss after epoch ', epoch, ': ', epoch_loss)\n",
    "\n",
    "    # print closest words to validation set at end of training\n",
    "\n",
    "    similarity_scores = tfs.run(similarity)\n",
    "    top_k = 5\n",
    "    for i in range(valid_size):\n",
    "        similar_words = (-similarity_scores[i, :]).argsort()[1:top_k + 1]\n",
    "        similar_str = 'Similar to {0:}:'.format(ptb.id2word[x_valid[i]])\n",
    "        for k in range(top_k):\n",
    "            similar_str = '{0:} {1:},'.format(\n",
    "                similar_str, ptb.id2word[similar_words[k]])\n",
    "        print(similar_str)\n",
    "\n",
    "    final_embeddings = tfs.run(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels):\n",
    "    assert low_dim_embs.shape[0] >= len(\n",
    "        labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2,\n",
    "            init='pca', n_iter=5000, method='exact')\n",
    "n_embeddings = 500\n",
    "low_dim_embeddings = tsne.fit_transform(final_embeddings[:n_embeddings, :])\n",
    "labels = [ptb.id2word[i] for i in range(n_embeddings)]\n",
    "\n",
    "plot_with_labels(low_dim_embeddings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings with Text8 data in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetslib.text8 import Text8\n",
    "text8 = Text8()\n",
    "# downloads data, converts words to ids, converts files to a list of ids\n",
    "text8.load_data()\n",
    "print('Train:', text8.part['train'][0:5])\n",
    "# print(text8.part['test'][0:5])\n",
    "# print(text8.part['valid'][0:5])\n",
    "print('Vocabulary Length = ', text8.vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8.skip_window = 2\n",
    "text8.reset_index()\n",
    "# in CBOW input is the context word and output is the target word\n",
    "y_batch, x_batch = text8.next_batch_cbow()\n",
    "\n",
    "print('The CBOW pairs : context,target')\n",
    "for i in range(5 * text8.skip_window):\n",
    "    print('(', [text8.id2word[x_i] for x_i in x_batch[i]],\n",
    "          ',', y_batch[i], text8.id2word[y_batch[i]], ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8.skip_window = 2\n",
    "text8.reset_index()\n",
    "# in skip-gram input is the target word and output is the context word\n",
    "x_batch, y_batch = text8.next_batch_sg()\n",
    "\n",
    "print('The skip-gram pairs : target,context')\n",
    "for i in range(5 * text8.skip_window):\n",
    "    print('(', x_batch[i], text8.id2word[x_batch[i]],\n",
    "          ',', y_batch[i], text8.id2word[y_batch[i]], ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 8\n",
    "x_valid = np.random.choice(valid_size * 10, valid_size, replace=False)\n",
    "print('valid: ',x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  \n",
    "n_negative_samples = 64\n",
    "text8.skip_window=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the effects of previous sessions in the Jupyter Notebook\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(dtype=tf.int32, shape=[batch_size])\n",
    "outputs = tf.placeholder(dtype=tf.int32, shape=[batch_size,1])\n",
    "inputs_valid = tf.constant(x_valid, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embeddings matrix with vocab_len rows and embedding_size columns\n",
    "# each row represents vectore representation or embedding of a word in the vocbulary\n",
    "\n",
    "embed_matrix = tf.Variable(tf.random_uniform(shape=[text8.vocab_len, embedding_size], \n",
    "                                           minval = -1.0, \n",
    "                                           maxval = 1.0\n",
    "                                          ),\n",
    "                           name='embed_matrix'\n",
    "                        )\n",
    "\n",
    "# define the embedding lookup table\n",
    "# provides the embeddings of the word ids in the input tensor\n",
    "embed_ltable = tf.nn.embedding_lookup(embed_matrix, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define noise-contrastive estimation (NCE) loss function\n",
    "\n",
    "nce_w = tf.Variable(tf.truncated_normal(shape=[text8.vocab_len, embedding_size],\n",
    "                                        stddev=1.0 / tf.sqrt(embedding_size*1.0)\n",
    "                                       )\n",
    "                   )\n",
    "nce_b = tf.Variable(tf.zeros(shape=[text8.vocab_len]))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_w,\n",
    "                                     biases=nce_b,\n",
    "                                     inputs=embed_ltable,\n",
    "                                     labels=outputs,\n",
    "                                     num_sampled=n_negative_samples,\n",
    "                                     num_classes=text8.vocab_len\n",
    "                                    )\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embed_matrix), 1, keep_dims=True))\n",
    "normalized_embeddings = embed_matrix / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, inputs_valid)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "learning_rate = 0.9\n",
    "text8.reset_index()\n",
    "n_batches = text8.n_batches_wv()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for step in range(n_batches):\n",
    "            x_batch, y_batch = text8.next_batch_sg()\n",
    "            y_batch = nputil.to2d(y_batch,unit_axis=1)\n",
    "            feed_dict = {inputs: x_batch, outputs: y_batch}\n",
    "\n",
    "            _, batch_loss = tfs.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            epoch_loss += batch_loss\n",
    "        epoch_loss = epoch_loss / n_batches \n",
    "\n",
    "        print('\\nAverage loss after epoch ', epoch, ': ', epoch_loss)\n",
    "        \n",
    "        # validating at end of every epoch\n",
    "\n",
    "    \n",
    "        similarity_scores = tfs.run(similarity)\n",
    "        for i in range(valid_size):\n",
    "            top_k = 5\n",
    "            similar_words = (-similarity_scores[i, :]).argsort()[1:top_k + 1]\n",
    "            similar_str = 'Similar to {0:}:'.format(text8.id2word[x_valid[i]])\n",
    "            for k in range(top_k):\n",
    "                similar_str = '{0:} {1:},'.format(similar_str, text8.id2word[similar_words[k]])\n",
    "            print(similar_str)\n",
    "        \n",
    "    final_embeddings = tfs.run(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_words(low_dim_embeddings, words):\n",
    "  assert low_dim_embeddings.shape[0] >= len(words), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, words in enumerate(words):\n",
    "    x, y = low_dim_embeddings[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(words,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "n_embeddings = 300\n",
    "low_dim_embeddings = tsne.fit_transform(final_embeddings[:n_embeddings, :])\n",
    "words = [text8.id2word[i] for i in range(n_embeddings)]\n",
    "  \n",
    "plot_with_words(low_dim_embeddings, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip-gram model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Input, Dense, Reshape, Dot, merge\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the jupyter buffers\n",
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 8\n",
    "x_valid = np.random.choice(valid_size * 10, valid_size, replace=False)\n",
    "print('valid: ',x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512  \n",
    "n_negative_samples = 64\n",
    "ptb.skip_window=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_table = sequence.make_sampling_table(ptb.vocab_len)\n",
    "pairs, labels= sequence.skipgrams(ptb.part['train'],\n",
    "                        ptb.vocab_len,\n",
    "                        window_size=ptb.skip_window,\n",
    "                        sampling_table=sample_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The skip-gram pairs : target,context')\n",
    "for i in range(5 * ptb.skip_window):\n",
    "    print(['{} {}'.format(id,ptb.id2word[id]) for id in pairs[i]],':',labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=zip(*pairs)\n",
    "x=np.array(x,dtype=np.int32)\n",
    "x=nputil.to2d(x,unit_axis=1)\n",
    "y=np.array(y,dtype=np.int32)\n",
    "y=nputil.to2d(y,unit_axis=1)\n",
    "labels=np.array(labels,dtype=np.int32)\n",
    "labels=nputil.to2d(labels,unit_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the target word model\n",
    "target_in = Input(shape=(1,),name='target_in')\n",
    "target = Embedding(ptb.vocab_len,embedding_size,input_length=1,name='target_em')(target_in)\n",
    "target = Reshape((embedding_size,1),name='target_re')(target)\n",
    "\n",
    "# build the context word model\n",
    "context_in = Input((1,),name='context_in')\n",
    "context = Embedding(ptb.vocab_len,embedding_size,input_length=1,name='context_em')(context_in)\n",
    "context = Reshape((embedding_size,1),name='context_re')(context)\n",
    "\n",
    "# merge the models with the dot product to check for similarity and add sigmoid layer\n",
    "output = Dot(axes=1,name='output_dot')([target,context])\n",
    "output = Reshape((1,),name='output_re')(output)\n",
    "output = Dense(1,activation='sigmoid',name='output_sig')(output)\n",
    "\n",
    "# create the functional model for finding word vectors\n",
    "model = Model(inputs=[target_in,context_in],outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# merge the models and create model to check for cosine similarity\n",
    "similarity = Dot(axes=0,normalize=True,name='sim_dot')([target,context])\n",
    "similarity_model = Model(inputs=[target_in,context_in],outputs=similarity)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "n_epochs = 5\n",
    "batch_size = 1024\n",
    "model.fit([x,y],labels,batch_size=batch_size, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print closest words to validation set at end of training\n",
    "top_k = 5  \n",
    "y_val = np.arange(ptb.vocab_len, dtype=np.int32)\n",
    "y_val = nputil.to2d(y_val,unit_axis=1)\n",
    "for i in range(valid_size):\n",
    "    x_val = np.full(shape=(ptb.vocab_len,1),fill_value=x_valid[i], dtype=np.int32)\n",
    "    similarity_scores = similarity_model.predict([x_val,y_val])\n",
    "    similarity_scores=similarity_scores.flatten()\n",
    "    similar_words = (-similarity_scores).argsort()[1:top_k + 1]\n",
    "    similar_str = 'Similar to {0:}:'.format(ptb.id2word[x_valid[i]])\n",
    "    for k in range(top_k):\n",
    "        similar_str = '{0:} {1:},'.format(similar_str, ptb.id2word[similar_words[k]])\n",
    "    print(similar_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec or embeddings visualisation using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the effects of previous sessions in the Jupyter Notebook\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  \n",
    "n_negative_samples = 64\n",
    "ptb.skip_window=2\n",
    "\n",
    "inputs = tf.placeholder(dtype=tf.int32, shape=[batch_size],name='inputs')\n",
    "outputs = tf.placeholder(dtype=tf.int32, shape=[batch_size,1],name='outputs')\n",
    "inputs_valid = tf.constant(x_valid, dtype=tf.int32,name='inputs_valid')\n",
    "\n",
    "# define embeddings matrix with vocab_len rows and embedding_size columns\n",
    "# each row represents vectore representation or embedding of a word in the vocbulary\n",
    "\n",
    "embed_matrix = tf.Variable(tf.random_uniform(shape=[ptb.vocab_len, embedding_size], \n",
    "                                           minval = -1.0, \n",
    "                                           maxval = 1.0\n",
    "                                          ),\n",
    "                           name='embed_matrix'\n",
    "                        )\n",
    "# define the embedding lookup table\n",
    "# provides the embeddings of the word ids in the input tensor\n",
    "embed_ltable = tf.nn.embedding_lookup(embed_matrix, inputs)\n",
    "\n",
    "# define noise-contrastive estimation (NCE) loss layer\n",
    "\n",
    "nce_w = tf.Variable(tf.truncated_normal(shape=[ptb.vocab_len, embedding_size],\n",
    "                                        stddev=1.0 / tf.sqrt(embedding_size*1.0)\n",
    "                                       ),\n",
    "                    name='nce_w'\n",
    "                   )\n",
    "nce_b = tf.Variable(tf.zeros(shape=[ptb.vocab_len]), name='nce_b')\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_w,\n",
    "                                     biases=nce_b,\n",
    "                                     inputs=embed_ltable,\n",
    "                                     labels=outputs,\n",
    "                                     num_sampled=n_negative_samples,\n",
    "                                     num_classes=ptb.vocab_len\n",
    "                                    ),\n",
    "                      name='nce_loss'\n",
    "                     )\n",
    "\n",
    "# Compute the cosine similarity between validation set samples and all embeddings.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embed_matrix), 1, keep_dims=True),name='norm')\n",
    "normalized_embeddings = tf.divide(embed_matrix,norm,name='normalized_embeddings')\n",
    "embed_valid = tf.nn.embedding_lookup(normalized_embeddings, inputs_valid)\n",
    "similarity = tf.matmul(embed_valid, normalized_embeddings, transpose_b=True, name='similarity')\n",
    "\n",
    "n_epochs = 10\n",
    "learning_rate = 0.9\n",
    "\n",
    "n_batches = ptb.n_batches_wv()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "log_dir = 'tflogs'\n",
    "\n",
    "# save the vocabulary\n",
    "vocabfile = 'word2id.tsv'\n",
    "ptb.save_word2id(os.path.join(log_dir, vocabfile))\n",
    "\n",
    "# create summary variable\n",
    "tf.summary.scalar('epoch_loss_scalar',epoch_loss)\n",
    "tf.summary.histogram('epoch_loss_histogram',epoch_loss)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "    \n",
    "\n",
    "with tf.Session() as tfs:\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Declare projector config\n",
    "    config=projector.ProjectorConfig()\n",
    "\n",
    "    # add embedding variable to the config\n",
    "    embed_conf1 = config.embeddings.add()\n",
    "    embed_conf1.tensor_name = embed_matrix.name\n",
    "    embed_conf1.metadata_path = vocabfile\n",
    "    \n",
    "    \n",
    "    file_writer = tf.summary.FileWriter(log_dir,tfs.graph)\n",
    "\n",
    "    # save embeddings config that TensorBoard will read and visualize\n",
    "    projector.visualize_embeddings(file_writer,config)\n",
    "    \n",
    "\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        ptb.reset_index()\n",
    "        for step in range(n_batches):\n",
    "            x_batch, y_batch = ptb.next_batch_sg()\n",
    "            y_batch = nputil.to2d(y_batch,unit_axis=1)\n",
    "            feed_dict = {inputs: x_batch, outputs: y_batch}\n",
    "            summary, _, batch_loss = tfs.run([merged, optimizer, loss], feed_dict=feed_dict)\n",
    "            epoch_loss += batch_loss\n",
    "            saver.save(tfs, \n",
    "                       os.path.join(log_dir, 'model.ckpt'), \n",
    "                       global_step = epoch * n_batches + step)\n",
    "        epoch_loss = epoch_loss / n_batches\n",
    "        file_writer.add_summary(summary,global_step = epoch * n_batches + step)\n",
    "\n",
    "        print('\\nAverage loss after epoch ', epoch, ': ', epoch_loss)\n",
    "        \n",
    "        # print closest words to validation set at end of every epoch\n",
    "\n",
    "        similarity_scores = tfs.run(similarity)\n",
    "        top_k = 5  \n",
    "        for i in range(valid_size):\n",
    "            similar_words = (-similarity_scores[i, :]).argsort()[1:top_k + 1]\n",
    "            similar_str = 'Similar to {0:}:'.format(ptb.id2word[x_valid[i]])\n",
    "            for k in range(top_k):\n",
    "                similar_str = '{0:} {1:},'.format(similar_str, ptb.id2word[similar_words[k]])\n",
    "            print(similar_str)\n",
    "        \n",
    "    final_embeddings = tfs.run(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The word2id and id2word code explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counter=collections.Counter(['a','b','a','c','a','c'])\n",
    "l=lambda x: (-x[1],x[0])\n",
    "count_pairs=sorted(counter.items(),key=l)\n",
    "words,_=list(zip(*count_pairs))\n",
    "print(words)\n",
    "\n",
    "word2id = dict(zip(words, range(len(words))))\n",
    "print(word2id)\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {
    "height": "202px",
    "width": "424px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
