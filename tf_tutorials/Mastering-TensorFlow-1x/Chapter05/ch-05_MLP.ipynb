{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes are from Fandango, Armando. Mastering TensorFlow 1.x: Advanced machine learning and deep learning concepts using TensorFlow 1.x and Keras (Kindle Locations 2090-2095). Packt Publishing. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#MNIST-Dataset\" data-toc-modified-id=\"MNIST-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MNIST Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-the-MNIST-data\" data-toc-modified-id=\"Get-the-MNIST-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get the MNIST data</a></span></li><li><span><a href=\"#MLP-in-TensorFlow\" data-toc-modified-id=\"MLP-in-TensorFlow-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>MLP in TensorFlow</a></span></li><li><span><a href=\"#MLP-in-Keras\" data-toc-modified-id=\"MLP-in-Keras-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>MLP in Keras</a></span></li><li><span><a href=\"#MLP-in-TFLearn\" data-toc-modified-id=\"MLP-in-TFLearn-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>MLP in TFLearn</a></span></li></ul></li><li><span><a href=\"#TimeSeries-Data---MLP---Keras\" data-toc-modified-id=\"TimeSeries-Data---MLP---Keras-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TimeSeries Data - MLP - Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-the-data\" data-toc-modified-id=\"Prepare-the-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Prepare the data</a></span></li><li><span><a href=\"#Build,-Train-and-Evaluate-the-Model\" data-toc-modified-id=\"Build,-Train-and-Evaluate-the-Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Build, Train and Evaluate the Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:1.14.5\n",
      "Pandas:0.22.0\n",
      "Matplotlib:2.2.2\n",
      "TensorFlow:1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras:2.2.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(\"NumPy:{}\".format(np.__version__))\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas:{}\".format(pd.__version__))\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "print(\"Matplotlib:{}\".format(mpl.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "print(\"TensorFlow:{}\".format(tf.__version__))\n",
    "\n",
    "import keras\n",
    "print(\"Keras:{}\".format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETSLIB_HOME = '../datasetslib'\n",
    "import sys\n",
    "if not DATASETSLIB_HOME in sys.path:\n",
    "    sys.path.append(DATASETSLIB_HOME)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import datasetslib\n",
    "\n",
    "from datasetslib import util as dsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetslib.datasets_root = os.path.join(os.path.expanduser('~'),'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-625c004abe30>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/ubuntu/datasets/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/ubuntu/datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/ubuntu/datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/ubuntu/datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(os.path.join(datasetslib.datasets_root, 'mnist'),\n",
    "                                  one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "Y_train = mnist.train.labels\n",
    "Y_test = mnist.test.labels\n",
    "print(X_train.shape,Y_train.shape)\n",
    "\n",
    "num_outputs = 10  # 0-9 digits\n",
    "num_inputs = 784  # total pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mlp() function builds the network layers with the following logic: \n",
    "\n",
    "The mlp() function takes five inputs: \n",
    "\n",
    "x is the input features tensor    \n",
    "num_inputs is the number of input features    \n",
    "num_outputs is the number of output targets    \n",
    "num_layers is the number of hidden layers required    \n",
    "num_neurons is the list containing the number of neurons for each layer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, num_inputs, num_outputs, num_layers, num_neurons):\n",
    "    w = []\n",
    "    b = []\n",
    "    #Run a loop for the number of hidden layers to create weights and bias tensors and append them to their respective lists:\n",
    "    #    The first dimension of the weight tensor is the number of inputs from the previous layer. \n",
    "    #    For the first hidden layer, the first dimension is num_inputs. \n",
    "    #    The second dimension of the weights tensor is the number of neurons in the current layer. \n",
    "    for i in range(num_layers):\n",
    "        # weights\n",
    "        #The tensors are given the names w_ < layer_num > and b_ < layer_num > respectively.\n",
    "        #The tensors are initialized with normal distribution using tf.random_normal(). \n",
    "        w.append(tf.Variable(tf.random_normal(\n",
    "            [num_inputs if i == 0 else num_neurons[i - 1],\n",
    "             num_neurons[i]]),\n",
    "            name=\"w_{0:04d}\".format(i)\n",
    "        ))\n",
    "        # biases\n",
    "        #    The biases are all one-dimensional tensors, where the dimension equals the number of neurons in the current layer.\n",
    "        b.append(tf.Variable(tf.random_normal(\n",
    "            [num_neurons[i]]),\n",
    "            name=\"b_{0:04d}\".format(i)\n",
    "        ))\n",
    "    # Create the weights and biases for the last hidden layer. \n",
    "    # The dimensions of the weights tensor are equal to the \n",
    "    # number of neurons in the last hidden layer and the number of output targets. \n",
    "    w.append(tf.Variable(tf.random_normal(\n",
    "        [num_neurons[num_layers - 1] if num_layers > 0 else num_inputs,\n",
    "         num_outputs]), name=\"w_out\"))\n",
    "    #The bias would be a tensor having a single dimension of the size of the number of output features:\n",
    "    b.append(tf.Variable(tf.random_normal([num_outputs]), name=\"b_out\"))\n",
    "\n",
    "    # Now start defining the layers. First, treat x as the first most visible input layer:\n",
    "    # x is input layer\n",
    "    layer = x\n",
    "    \n",
    "    #Add the hidden layers in a loop. \n",
    "    # Each hidden layer represents the linear function \n",
    "    # tf.matmul( layer, w[ i]) + b[ i] being made nonlinear by the activation function tf.nn.relu():\n",
    "    # add hidden layers\n",
    "    for i in range(num_layers):\n",
    "        layer = tf.nn.relu(tf.matmul(layer, w[i]) + b[i])\n",
    "    \n",
    "    # add output layer\n",
    "    # The one difference between the output layer and the hidden layer is the \n",
    "    # absence of activation function in the output layer:\n",
    "    layer = tf.matmul(layer, w[num_layers]) + b[num_layers]\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The helper function mnist_batch_func() wraps the TensorFlow's batch function for the MNIST dataset \n",
    "#  to provide the next batch of images:\n",
    "#TensorFlow provides this function for the MNIST dataset; \n",
    "#  however, for other datasets, we may have to write our own batch function.\n",
    "\n",
    "def mnist_batch_func(batch_size=100):\n",
    "    X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "    return [X_batch, Y_batch]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function, tensorflow_classification(), trains and evaluates the model. \n",
    "\n",
    "The tensorflow_classification() function takes several inputs: \n",
    "\n",
    "* n_epochs is the number of training loops to run \n",
    "* n_batches is the number of randomly sampled batches for which the training in each cycle should be run \n",
    "* batch_size is the number of samples in each batch \n",
    "* batch_func is the function that takes the batch_size and returns the sample batch of X and Y \n",
    "* model is the actual neural network or layers with neurons \n",
    "* optimizer is the optimization function defined using TensorFlow \n",
    "* loss is the loss of cost function that the optimizer would optimize the parameters for \n",
    "* accuracy_function is the function that calculates the accuracy score \n",
    "* X_test and Y_test are the datasets for the testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_classification(n_epochs, n_batches,\n",
    "                              batch_size, batch_func,\n",
    "                              model, optimizer, loss, accuracy_function,\n",
    "                              X_test, Y_test):\n",
    "    with tf.Session() as tfs:\n",
    "        tfs.run(tf.global_variables_initializer())\n",
    "        #Run the training for n_epoch cycles:\n",
    "        for epoch in range(n_epochs):\n",
    "            #In each cycle, take the n_batches number of sample sets and train the model, \n",
    "            #calculate the loss for each batch, calculate the average loss for each epoch:\n",
    "            epoch_loss = 0.0\n",
    "            for batch in range(n_batches):\n",
    "                X_batch, Y_batch = batch_func(batch_size)\n",
    "                feed_dict = {x: X_batch, y: Y_batch}\n",
    "                _, batch_loss = tfs.run([optimizer, loss], feed_dict)\n",
    "                epoch_loss += batch_loss\n",
    "            average_loss = epoch_loss / n_batches\n",
    "            print(\"epoch: {0:04d}   loss = {1:0.6f}\".format(\n",
    "                epoch, average_loss))\n",
    "        #When all the epoch cycles are finished, calculate and print the accuracy score calculated with the accuracy_function:\n",
    "        feed_dict = {x: X_test, y: Y_test}\n",
    "        accuracy_score = tfs.run(accuracy_function, feed_dict=feed_dict)\n",
    "        print(\"accuracy={0:.8f}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are described below: \n",
    "\n",
    "* num_layers is the number of hidden layers. We first practice with no hidden layer, only the input, and output layers. \n",
    "\n",
    "* num_neurons is the empty list because there are no hidden layers. \n",
    "* learning_rate is 0.01, a randomly selected small number. \n",
    "\n",
    "* num_epochs represents the 50 iterations to learn the parameters for the only neuron that connects the inputs to the output. \n",
    "\n",
    "* batch_size is kept at 100, again a matter of choice. Larger batch size does not necessarily offer higher benefits. You might have to explore different batch sizes to find the optimum batch size for your neural networks. \n",
    "\n",
    "* n_batches: Number of batches is calculated approximately to be the number of examples divided by the number of samples in a batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 0\n",
    "num_neurons = []\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs])\n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp(x=x,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            num_layers=num_layers,\n",
    "            num_neurons=num_neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the softmax_cross_entropy_with_logits() function is used, make sure that the output is unscaled and has not been passed through the softmax activation function. This function internally uses softmax to scale the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-d0a8587a25df>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the softmax entropy between the model (the estimated value y) and the actual value of y. \n",
    "\n",
    "The entropy function is used when the output belongs to one class and not more than one class. \n",
    "\n",
    "As in our example, the image can only belong to one of the digits. \n",
    "\n",
    "More information on this entropy function can be found at https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer function\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code might run for a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 8.473080\n",
      "epoch: 0001   loss = 4.390884\n",
      "epoch: 0002   loss = 3.030011\n",
      "epoch: 0003   loss = 2.392622\n",
      "epoch: 0004   loss = 2.023636\n",
      "epoch: 0005   loss = 1.782972\n",
      "epoch: 0006   loss = 1.613726\n",
      "epoch: 0007   loss = 1.487805\n",
      "epoch: 0008   loss = 1.390011\n",
      "epoch: 0009   loss = 1.311847\n",
      "epoch: 0010   loss = 1.247507\n",
      "epoch: 0011   loss = 1.193481\n",
      "epoch: 0012   loss = 1.147407\n",
      "epoch: 0013   loss = 1.107349\n",
      "epoch: 0014   loss = 1.072285\n",
      "epoch: 0015   loss = 1.041205\n",
      "epoch: 0016   loss = 1.013279\n",
      "epoch: 0017   loss = 0.988314\n",
      "epoch: 0018   loss = 0.965635\n",
      "epoch: 0019   loss = 0.944805\n",
      "epoch: 0020   loss = 0.925856\n",
      "epoch: 0021   loss = 0.908260\n",
      "epoch: 0022   loss = 0.891954\n",
      "epoch: 0023   loss = 0.876993\n",
      "epoch: 0024   loss = 0.862779\n",
      "epoch: 0025   loss = 0.849480\n",
      "epoch: 0026   loss = 0.837104\n",
      "epoch: 0027   loss = 0.825333\n",
      "epoch: 0028   loss = 0.814350\n",
      "epoch: 0029   loss = 0.803741\n",
      "epoch: 0030   loss = 0.793860\n",
      "epoch: 0031   loss = 0.784387\n",
      "epoch: 0032   loss = 0.775280\n",
      "epoch: 0033   loss = 0.766651\n",
      "epoch: 0034   loss = 0.758485\n",
      "epoch: 0035   loss = 0.750555\n",
      "epoch: 0036   loss = 0.743002\n",
      "epoch: 0037   loss = 0.735639\n",
      "epoch: 0038   loss = 0.728810\n",
      "epoch: 0039   loss = 0.722074\n",
      "epoch: 0040   loss = 0.715607\n",
      "epoch: 0041   loss = 0.709368\n",
      "epoch: 0042   loss = 0.703364\n",
      "epoch: 0043   loss = 0.697576\n",
      "epoch: 0044   loss = 0.691993\n",
      "epoch: 0045   loss = 0.686521\n",
      "epoch: 0046   loss = 0.681212\n",
      "epoch: 0047   loss = 0.676204\n",
      "epoch: 0048   loss = 0.671218\n",
      "epoch: 0049   loss = 0.666440\n",
      "accuracy=0.85869998\n"
     ]
    }
   ],
   "source": [
    "tensorflow_classification(n_epochs=n_epochs,\n",
    "                          n_batches=n_batches,\n",
    "                          batch_size=batch_size,\n",
    "                          batch_func=mnist_batch_func,\n",
    "                          model=model,\n",
    "                          optimizer=optimizer,\n",
    "                          loss=loss,\n",
    "                          accuracy_function=accuracy_function,\n",
    "                          X_test=mnist.test.images,\n",
    "                          Y_test=mnist.test.labels\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the single neuron network slowly reduces the loss from 8.47 to 0.66 over 50 iterations, finally getting an accuracy of almost 85 percent. This is pretty bad accuracy for this specific example because this was only a demonstration of using TensorFlow for classification using MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 4.013396\n",
      "epoch: 0001   loss = 2.302745\n",
      "epoch: 0002   loss = 2.156557\n",
      "epoch: 0003   loss = 2.079378\n",
      "epoch: 0004   loss = 2.026683\n",
      "epoch: 0005   loss = 1.987903\n",
      "epoch: 0006   loss = 1.958130\n",
      "epoch: 0007   loss = 1.932892\n",
      "epoch: 0008   loss = 1.909715\n",
      "epoch: 0009   loss = 1.887257\n",
      "epoch: 0010   loss = 1.865864\n",
      "epoch: 0011   loss = 1.844288\n",
      "epoch: 0012   loss = 1.823081\n",
      "epoch: 0013   loss = 1.801307\n",
      "epoch: 0014   loss = 1.778486\n",
      "epoch: 0015   loss = 1.754043\n",
      "epoch: 0016   loss = 1.726848\n",
      "epoch: 0017   loss = 1.695823\n",
      "epoch: 0018   loss = 1.662781\n",
      "epoch: 0019   loss = 1.630441\n",
      "epoch: 0020   loss = 1.598397\n",
      "epoch: 0021   loss = 1.568061\n",
      "epoch: 0022   loss = 1.537702\n",
      "epoch: 0023   loss = 1.508838\n",
      "epoch: 0024   loss = 1.480578\n",
      "epoch: 0025   loss = 1.452838\n",
      "epoch: 0026   loss = 1.426458\n",
      "epoch: 0027   loss = 1.400067\n",
      "epoch: 0028   loss = 1.375169\n",
      "epoch: 0029   loss = 1.351013\n",
      "epoch: 0030   loss = 1.326956\n",
      "epoch: 0031   loss = 1.303237\n",
      "epoch: 0032   loss = 1.281651\n",
      "epoch: 0033   loss = 1.259432\n",
      "epoch: 0034   loss = 1.238814\n",
      "epoch: 0035   loss = 1.218468\n",
      "epoch: 0036   loss = 1.198567\n",
      "epoch: 0037   loss = 1.179669\n",
      "epoch: 0038   loss = 1.161184\n",
      "epoch: 0039   loss = 1.145037\n",
      "epoch: 0040   loss = 1.129737\n",
      "epoch: 0041   loss = 1.114725\n",
      "epoch: 0042   loss = 1.101283\n",
      "epoch: 0043   loss = 1.088011\n",
      "epoch: 0044   loss = 1.076294\n",
      "epoch: 0045   loss = 1.065239\n",
      "epoch: 0046   loss = 1.054565\n",
      "epoch: 0047   loss = 1.045264\n",
      "epoch: 0048   loss = 1.035671\n",
      "epoch: 0049   loss = 1.027042\n",
      "accuracy=0.65469998\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1 \n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(8)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n",
    "\n",
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "# optimizer function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = mnist.test.images, \n",
    "                          Y_test = mnist.test.labels\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the same code again by adding 256 more hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, \n",
    "                                                              labels=y))\n",
    "# optimizer function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code might take a few minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 58.754058\n",
      "epoch: 0001   loss = 14.521847\n",
      "epoch: 0002   loss = 8.922171\n",
      "epoch: 0003   loss = 6.241642\n",
      "epoch: 0004   loss = 4.599127\n",
      "epoch: 0005   loss = 3.614644\n",
      "epoch: 0006   loss = 2.846957\n",
      "epoch: 0007   loss = 2.325747\n",
      "epoch: 0008   loss = 1.888454\n",
      "epoch: 0009   loss = 1.571295\n",
      "epoch: 0010   loss = 1.307724\n",
      "epoch: 0011   loss = 1.085919\n",
      "epoch: 0012   loss = 0.910493\n",
      "epoch: 0013   loss = 0.756905\n",
      "epoch: 0014   loss = 0.661583\n",
      "epoch: 0015   loss = 0.557197\n",
      "epoch: 0016   loss = 0.463660\n",
      "epoch: 0017   loss = 0.398142\n",
      "epoch: 0018   loss = 0.337610\n",
      "epoch: 0019   loss = 0.281566\n",
      "epoch: 0020   loss = 0.236837\n",
      "epoch: 0021   loss = 0.207340\n",
      "epoch: 0022   loss = 0.166393\n",
      "epoch: 0023   loss = 0.147394\n",
      "epoch: 0024   loss = 0.126139\n",
      "epoch: 0025   loss = 0.107418\n",
      "epoch: 0026   loss = 0.088536\n",
      "epoch: 0027   loss = 0.066566\n",
      "epoch: 0028   loss = 0.055672\n",
      "epoch: 0029   loss = 0.056304\n",
      "epoch: 0030   loss = 0.042222\n",
      "epoch: 0031   loss = 0.032222\n",
      "epoch: 0032   loss = 0.028842\n",
      "epoch: 0033   loss = 0.028369\n",
      "epoch: 0034   loss = 0.023477\n",
      "epoch: 0035   loss = 0.012445\n",
      "epoch: 0036   loss = 0.011120\n",
      "epoch: 0037   loss = 0.010939\n",
      "epoch: 0038   loss = 0.008139\n",
      "epoch: 0039   loss = 0.006357\n",
      "epoch: 0040   loss = 0.003564\n",
      "epoch: 0041   loss = 0.004040\n",
      "epoch: 0042   loss = 0.002573\n",
      "epoch: 0043   loss = 0.002124\n",
      "epoch: 0044   loss = 0.001201\n",
      "epoch: 0045   loss = 0.001176\n",
      "epoch: 0046   loss = 0.000473\n",
      "epoch: 0047   loss = 0.000041\n",
      "epoch: 0048   loss = 0.000006\n",
      "epoch: 0049   loss = 0.000005\n",
      "accuracy=0.93510002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = mnist.test.images, \n",
    "                          Y_test = mnist.test.labels\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, by adding two rows and 256 neurons to each layer, we brought the accuracy up to 0.936. You are encouraged to try the code with different values of variables to observe how it affects the loss and accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the same MLP network with Keras, a high-level library for TensorFlow. We keep all the parameters the same as we used for the TensorFlow example in this chapter, for example, the activation function for the hidden layers is kept as the ReLU function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer. Only in the first hidden layer, we have to specify the shape of the input tensor:\n",
    "model.add(Dense(units=num_neurons[0], activation='relu', \n",
    "                input_shape=(num_inputs,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(units=num_neurons[1], activation='relu'))\n",
    "\n",
    "# Add the output layer with the activation function softmax:\n",
    "model.add(Dense(units=num_outputs, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model with an SGD optimizer:\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 2s 36us/step - loss: 1.1228 - acc: 0.7337\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.4494 - acc: 0.8800\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.3570 - acc: 0.9002\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.3158 - acc: 0.9106\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2892 - acc: 0.9182\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2693 - acc: 0.9230\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2533 - acc: 0.9276\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2392 - acc: 0.9324\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2270 - acc: 0.9357\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2164 - acc: 0.9383\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2063 - acc: 0.9418\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1973 - acc: 0.9437\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1892 - acc: 0.9468\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1814 - acc: 0.9488\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1744 - acc: 0.9507\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1678 - acc: 0.9528\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1616 - acc: 0.9540\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1559 - acc: 0.9564\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1506 - acc: 0.9580\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1454 - acc: 0.9593\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1408 - acc: 0.9606\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1362 - acc: 0.9618\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.1321 - acc: 0.9628\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1280 - acc: 0.9640\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1243 - acc: 0.9656\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1205 - acc: 0.9664\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1173 - acc: 0.9673\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1140 - acc: 0.9683\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1107 - acc: 0.9694\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1079 - acc: 0.9702\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1049 - acc: 0.9708\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1022 - acc: 0.9721\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0997 - acc: 0.9729\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0972 - acc: 0.9733\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0949 - acc: 0.9740\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0923 - acc: 0.9746\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0903 - acc: 0.9751\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0880 - acc: 0.9759\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0858 - acc: 0.9767\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0839 - acc: 0.9774\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0821 - acc: 0.9777\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0801 - acc: 0.9782\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0787 - acc: 0.9784\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0767 - acc: 0.9794\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0752 - acc: 0.9794\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0735 - acc: 0.9802\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0721 - acc: 0.9807\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0705 - acc: 0.9810\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0688 - acc: 0.9818\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0675 - acc: 0.9822\n",
      "10000/10000 [==============================] - 0s 40us/step\n",
      "\n",
      "Test loss: 0.09095124787967653\n",
      "Test accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in TFLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to implement the same MLP using TFLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build deep neural network\n",
    "# input layer, two hidden layers, and the output layer \n",
    "# (the same architecture as examples in TensorFlow and Keras sections):\n",
    "\n",
    "input_layer = tflearn.input_data(shape=[None, num_inputs])\n",
    "dense1 = tflearn.fully_connected(input_layer, num_neurons[0], activation='relu')\n",
    "dense2 = tflearn.fully_connected(dense1, num_neurons[1], activation='relu')\n",
    "softmax = tflearn.fully_connected(dense2, num_outputs, activation='softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tflearn.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "net = tflearn.regression(softmax, optimizer=optimizer, \n",
    "                         metric=tflearn.metrics.Accuracy(), \n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MLP model is known as DNN in TFLearn\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 27499  | total loss: \u001b[1m\u001b[32m0.12371\u001b[0m\u001b[0m | time: 2.771s\n",
      "| SGD | epoch: 050 | loss: 0.12371 - acc: 0.9628 -- iter: 54900/55000\n",
      "Training Step: 27500  | total loss: \u001b[1m\u001b[32m0.11805\u001b[0m\u001b[0m | time: 2.777s\n",
      "| SGD | epoch: 050 | loss: 0.11805 - acc: 0.9655 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(X_train, Y_train, \n",
    "          n_epoch=n_epochs, batch_size=batch_size, \n",
    "          show_metric=True, run_id='dense_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9647\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeries Data - MLP - Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/datasets'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetslib.datasets_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#dataframe = pd.read_csv(os.path.join(datasetslib.datasets_root, \n",
    "#                                     'ts-data', \n",
    "#                                     '../data/international-airline-passengers.csv'), \n",
    "#                        usecols=[1],header=0)\n",
    "\n",
    "# Let us use panda to read the data into a dataframe\n",
    "dataframe = pd.read_csv('../data/international-airline-passengers.csv', \n",
    "                        usecols=[1],header=0)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 48\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "# For time series datasets, we have a separate function that does not shuffle the observations because \n",
    "# for time series regression we need to maintain the order of the observations. \n",
    "# We use 67 percent data for training and 33 percent for testing.\n",
    "\n",
    "train,test=dsu.train_test_split(dataset,train_size=0.67)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time series regression, we convert the dataset to build a supervised data set. \n",
    "\n",
    "We use a lag of two time steps in this example. \n",
    "\n",
    "We set n_x to 2 and the mvts_to_xy() function returns the input and output (X and Y) train and test sets such that X has values for time {t-1, t} in two columns and Y has values for time {t + 1} in one column. \n",
    "\n",
    "Our learning algorithm assumes that values at time t + 1 can be learned by finding the relationship between values for time {t-1, t, t + 1}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t-1,t and Y=t+1\n",
    "# TimeSeries to XY\n",
    "n_x=2\n",
    "n_y=1\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = dsu.mvts_to_xy(train,test,n_x=n_x,n_y=n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on converting time series datasets as supervised learning problems can be found at the following link: \n",
    "http://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 2) (95, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "num_neurons = [8,8]\n",
    "n_epochs = 50\n",
    "batch_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 105\n",
      "Trainable params: 105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(num_neurons[0], activation='relu', input_shape=(n_x,)))\n",
    "model.add(Dense(num_neurons[1], activation='relu'))\n",
    "model.add(Dense(units=1))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with MSE loss and Adam optimizer\n",
    "model.compile(loss='mse', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "95/95 [==============================] - 0s 4ms/step - loss: 89630.0523\n",
      "Epoch 2/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 65499.0244\n",
      "Epoch 3/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 52652.4532\n",
      "Epoch 4/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 49833.5211\n",
      "Epoch 5/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 46130.1543\n",
      "Epoch 6/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 38998.8676\n",
      "Epoch 7/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 28291.7857\n",
      "Epoch 8/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 16052.6664\n",
      "Epoch 9/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 6496.2310\n",
      "Epoch 10/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 2046.9626\n",
      "Epoch 11/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 967.6489\n",
      "Epoch 12/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 825.5313\n",
      "Epoch 13/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 826.3555\n",
      "Epoch 14/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 816.2632\n",
      "Epoch 15/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 832.3412\n",
      "Epoch 16/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 814.3739\n",
      "Epoch 17/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 831.2007\n",
      "Epoch 18/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 816.4391\n",
      "Epoch 19/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 820.1573\n",
      "Epoch 20/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 819.5567\n",
      "Epoch 21/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 812.5090\n",
      "Epoch 22/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 810.1774\n",
      "Epoch 23/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 814.7811\n",
      "Epoch 24/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 810.3038\n",
      "Epoch 25/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 808.7533\n",
      "Epoch 26/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 810.5137\n",
      "Epoch 27/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 821.3369\n",
      "Epoch 28/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 818.5457\n",
      "Epoch 29/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 806.7286\n",
      "Epoch 30/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 819.1024\n",
      "Epoch 31/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 809.2894\n",
      "Epoch 32/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 816.4314\n",
      "Epoch 33/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 811.5708\n",
      "Epoch 34/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 809.9102\n",
      "Epoch 35/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 828.6766\n",
      "Epoch 36/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 818.9131\n",
      "Epoch 37/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 805.5200\n",
      "Epoch 38/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 808.1772\n",
      "Epoch 39/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 810.4909\n",
      "Epoch 40/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 801.1624\n",
      "Epoch 41/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 802.1038\n",
      "Epoch 42/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 805.8681\n",
      "Epoch 43/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 809.9543\n",
      "Epoch 44/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 798.9935\n",
      "Epoch 45/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 800.5617\n",
      "Epoch 46/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 815.1320\n",
      "Epoch 47/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 828.0239\n",
      "Epoch 48/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 826.5135\n",
      "Epoch 49/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 807.7745\n",
      "Epoch 50/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 793.9005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93ad3d7eb8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 2) (46, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the training set and test set\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "Y_train_pred_plot = np.empty_like(dataset)\n",
    "Y_train_pred_plot[:, :] = np.nan\n",
    "Y_train_pred_plot[n_x-1:len(Y_train_pred)+n_x-1, :] = Y_train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift test predictions for plotting\n",
    "Y_test_pred_plot = np.empty_like(dataset)\n",
    "Y_test_pred_plot[:, :] = np.nan\n",
    "Y_test_pred_plot[len(Y_train_pred)+(n_x*2)-1:len(dataset)-1, :] = Y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot baseline and predictions\n",
    "plt.plot(dataset,label='Original Data')\n",
    "plt.plot(Y_train_pred_plot,label='Y_train_pred')\n",
    "plt.plot(Y_test_pred_plot,label='Y_test_pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
