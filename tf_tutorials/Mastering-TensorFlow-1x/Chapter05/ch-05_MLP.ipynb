{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#MNIST-Dataset\" data-toc-modified-id=\"MNIST-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MNIST Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-the-MNIST-data\" data-toc-modified-id=\"Get-the-MNIST-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get the MNIST data</a></span></li><li><span><a href=\"#MLP-in-TensorFlow\" data-toc-modified-id=\"MLP-in-TensorFlow-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>MLP in TensorFlow</a></span></li><li><span><a href=\"#MLP-in-Keras\" data-toc-modified-id=\"MLP-in-Keras-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>MLP in Keras</a></span></li><li><span><a href=\"#MLP-in-TFLearn\" data-toc-modified-id=\"MLP-in-TFLearn-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>MLP in TFLearn</a></span></li></ul></li><li><span><a href=\"#TimeSeries-Data---MLP---Keras\" data-toc-modified-id=\"TimeSeries-Data---MLP---Keras-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TimeSeries Data - MLP - Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-the-data\" data-toc-modified-id=\"Prepare-the-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Prepare the data</a></span></li><li><span><a href=\"#Build,-Train-and-Evaluate-the-Model\" data-toc-modified-id=\"Build,-Train-and-Evaluate-the-Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Build, Train and Evaluate the Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:1.14.5\n",
      "Pandas:0.22.0\n",
      "Matplotlib:2.2.2\n",
      "TensorFlow:1.11.0\n",
      "Keras:2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(\"NumPy:{}\".format(np.__version__))\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas:{}\".format(pd.__version__))\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "print(\"Matplotlib:{}\".format(mpl.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "print(\"TensorFlow:{}\".format(tf.__version__))\n",
    "\n",
    "import keras\n",
    "print(\"Keras:{}\".format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETSLIB_HOME = '../datasetslib'\n",
    "import sys\n",
    "if not DATASETSLIB_HOME in sys.path:\n",
    "    sys.path.append(DATASETSLIB_HOME)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import datasetslib\n",
    "\n",
    "from datasetslib import util as dsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetslib.datasets_root = os.path.join(os.path.expanduser('~'),'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-0d8b9aad8e53>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/ubuntu/datasets/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/ubuntu/datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/ubuntu/datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/ubuntu/datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(os.path.join(datasetslib.datasets_root, 'mnist'),\n",
    "                                  one_hot=True)\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "Y_train = mnist.train.labels\n",
    "Y_test = mnist.test.labels\n",
    "\n",
    "num_outputs = 10  # 0-9 digits\n",
    "num_inputs = 784  # total pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, num_inputs, num_outputs, num_layers, num_neurons):\n",
    "    w = []\n",
    "    b = []\n",
    "    for i in range(num_layers):\n",
    "        # weights\n",
    "        w.append(tf.Variable(tf.random_normal(\n",
    "            [num_inputs if i == 0 else num_neurons[i - 1],\n",
    "             num_neurons[i]]),\n",
    "            name=\"w_{0:04d}\".format(i)\n",
    "        ))\n",
    "        # biases\n",
    "        b.append(tf.Variable(tf.random_normal(\n",
    "            [num_neurons[i]]),\n",
    "            name=\"b_{0:04d}\".format(i)\n",
    "        ))\n",
    "    w.append(tf.Variable(tf.random_normal(\n",
    "        [num_neurons[num_layers - 1] if num_layers > 0 else num_inputs,\n",
    "         num_outputs]), name=\"w_out\"))\n",
    "    b.append(tf.Variable(tf.random_normal([num_outputs]), name=\"b_out\"))\n",
    "\n",
    "    # x is input layer\n",
    "    layer = x\n",
    "    # add hidden layers\n",
    "    for i in range(num_layers):\n",
    "        layer = tf.nn.relu(tf.matmul(layer, w[i]) + b[i])\n",
    "    # add output layer\n",
    "    layer = tf.matmul(layer, w[num_layers]) + b[num_layers]\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def mnist_batch_func(batch_size=100):\n",
    "    X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "    return [X_batch, Y_batch]\n",
    "\n",
    "\n",
    "def tensorflow_classification(n_epochs, n_batches,\n",
    "                              batch_size, batch_func,\n",
    "                              model, optimizer, loss, accuracy_function,\n",
    "                              X_test, Y_test):\n",
    "    with tf.Session() as tfs:\n",
    "        tfs.run(tf.global_variables_initializer())\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch in range(n_batches):\n",
    "                X_batch, Y_batch = batch_func(batch_size)\n",
    "                feed_dict = {x: X_batch, y: Y_batch}\n",
    "                _, batch_loss = tfs.run([optimizer, loss], feed_dict)\n",
    "                epoch_loss += batch_loss\n",
    "            average_loss = epoch_loss / n_batches\n",
    "            print(\"epoch: {0:04d}   loss = {1:0.6f}\".format(\n",
    "                epoch, average_loss))\n",
    "        feed_dict = {x: X_test, y: Y_test}\n",
    "        accuracy_score = tfs.run(accuracy_function, feed_dict=feed_dict)\n",
    "        print(\"accuracy={0:.8f}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-d531aa8ea8fb>:21: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "epoch: 0000   loss = 7.310428\n",
      "epoch: 0001   loss = 4.130082\n",
      "epoch: 0002   loss = 2.953550\n",
      "epoch: 0003   loss = 2.362319\n",
      "epoch: 0004   loss = 2.018068\n",
      "epoch: 0005   loss = 1.792703\n",
      "epoch: 0006   loss = 1.632227\n",
      "epoch: 0007   loss = 1.511098\n",
      "epoch: 0008   loss = 1.415811\n",
      "epoch: 0009   loss = 1.338710\n",
      "epoch: 0010   loss = 1.274726\n",
      "epoch: 0011   loss = 1.220370\n",
      "epoch: 0012   loss = 1.173759\n",
      "epoch: 0013   loss = 1.132906\n",
      "epoch: 0014   loss = 1.096913\n",
      "epoch: 0015   loss = 1.064849\n",
      "epoch: 0016   loss = 1.035915\n",
      "epoch: 0017   loss = 1.009694\n",
      "epoch: 0018   loss = 0.985728\n",
      "epoch: 0019   loss = 0.963920\n",
      "epoch: 0020   loss = 0.943645\n",
      "epoch: 0021   loss = 0.924987\n",
      "epoch: 0022   loss = 0.907552\n",
      "epoch: 0023   loss = 0.891403\n",
      "epoch: 0024   loss = 0.876160\n",
      "epoch: 0025   loss = 0.861992\n",
      "epoch: 0026   loss = 0.848590\n",
      "epoch: 0027   loss = 0.835960\n",
      "epoch: 0028   loss = 0.823992\n",
      "epoch: 0029   loss = 0.812649\n",
      "epoch: 0030   loss = 0.801958\n",
      "epoch: 0031   loss = 0.791785\n",
      "epoch: 0032   loss = 0.781988\n",
      "epoch: 0033   loss = 0.772692\n",
      "epoch: 0034   loss = 0.763890\n",
      "epoch: 0035   loss = 0.755422\n",
      "epoch: 0036   loss = 0.747271\n",
      "epoch: 0037   loss = 0.739446\n",
      "epoch: 0038   loss = 0.732083\n",
      "epoch: 0039   loss = 0.724911\n",
      "epoch: 0040   loss = 0.717956\n",
      "epoch: 0041   loss = 0.711281\n",
      "epoch: 0042   loss = 0.704942\n",
      "epoch: 0043   loss = 0.698759\n",
      "epoch: 0044   loss = 0.692831\n",
      "epoch: 0045   loss = 0.687025\n",
      "epoch: 0046   loss = 0.681479\n",
      "epoch: 0047   loss = 0.676149\n",
      "epoch: 0048   loss = 0.670917\n",
      "epoch: 0049   loss = 0.665820\n",
      "accuracy=0.86089998\n"
     ]
    }
   ],
   "source": [
    "num_layers = 0\n",
    "num_neurons = []\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs])\n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs])\n",
    "\n",
    "model = mlp(x=x,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            num_layers=num_layers,\n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "# optimizer function\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs,\n",
    "                          n_batches=n_batches,\n",
    "                          batch_size=batch_size,\n",
    "                          batch_func=mnist_batch_func,\n",
    "                          model=model,\n",
    "                          optimizer=optimizer,\n",
    "                          loss=loss,\n",
    "                          accuracy_function=accuracy_function,\n",
    "                          X_test=mnist.test.images,\n",
    "                          Y_test=mnist.test.labels\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 3.982879\n",
      "epoch: 0001   loss = 2.290242\n",
      "epoch: 0002   loss = 2.146824\n",
      "epoch: 0003   loss = 2.049216\n",
      "epoch: 0004   loss = 1.977785\n",
      "epoch: 0005   loss = 1.922585\n",
      "epoch: 0006   loss = 1.875601\n",
      "epoch: 0007   loss = 1.832573\n",
      "epoch: 0008   loss = 1.790754\n",
      "epoch: 0009   loss = 1.748355\n",
      "epoch: 0010   loss = 1.704254\n",
      "epoch: 0011   loss = 1.657746\n",
      "epoch: 0012   loss = 1.609882\n",
      "epoch: 0013   loss = 1.561166\n",
      "epoch: 0014   loss = 1.514168\n",
      "epoch: 0015   loss = 1.469856\n",
      "epoch: 0016   loss = 1.428373\n",
      "epoch: 0017   loss = 1.388716\n",
      "epoch: 0018   loss = 1.349924\n",
      "epoch: 0019   loss = 1.313423\n",
      "epoch: 0020   loss = 1.277759\n",
      "epoch: 0021   loss = 1.244732\n",
      "epoch: 0022   loss = 1.213374\n",
      "epoch: 0023   loss = 1.186042\n",
      "epoch: 0024   loss = 1.161652\n",
      "epoch: 0025   loss = 1.138693\n",
      "epoch: 0026   loss = 1.118391\n",
      "epoch: 0027   loss = 1.099992\n",
      "epoch: 0028   loss = 1.082146\n",
      "epoch: 0029   loss = 1.066611\n",
      "epoch: 0030   loss = 1.051637\n",
      "epoch: 0031   loss = 1.036788\n",
      "epoch: 0032   loss = 1.023856\n",
      "epoch: 0033   loss = 1.011050\n",
      "epoch: 0034   loss = 0.999343\n",
      "epoch: 0035   loss = 0.987206\n",
      "epoch: 0036   loss = 0.975864\n",
      "epoch: 0037   loss = 0.965143\n",
      "epoch: 0038   loss = 0.954377\n",
      "epoch: 0039   loss = 0.944477\n",
      "epoch: 0040   loss = 0.934416\n",
      "epoch: 0041   loss = 0.924550\n",
      "epoch: 0042   loss = 0.915341\n",
      "epoch: 0043   loss = 0.905662\n",
      "epoch: 0044   loss = 0.896826\n",
      "epoch: 0045   loss = 0.888184\n",
      "epoch: 0046   loss = 0.878426\n",
      "epoch: 0047   loss = 0.870259\n",
      "epoch: 0048   loss = 0.862218\n",
      "epoch: 0049   loss = 0.853487\n",
      "accuracy=0.73210001\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1 \n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(8)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n",
    "\n",
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "# optimizer function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = mnist.test.images, \n",
    "                          Y_test = mnist.test.labels\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 59.286007\n",
      "epoch: 0001   loss = 14.451488\n",
      "epoch: 0002   loss = 8.853264\n",
      "epoch: 0003   loss = 6.265361\n",
      "epoch: 0004   loss = 4.805574\n",
      "epoch: 0005   loss = 3.763518\n",
      "epoch: 0006   loss = 3.057002\n",
      "epoch: 0007   loss = 2.483716\n",
      "epoch: 0008   loss = 2.039015\n",
      "epoch: 0009   loss = 1.686357\n",
      "epoch: 0010   loss = 1.451302\n",
      "epoch: 0011   loss = 1.231050\n",
      "epoch: 0012   loss = 1.050846\n",
      "epoch: 0013   loss = 0.843715\n",
      "epoch: 0014   loss = 0.764786\n",
      "epoch: 0015   loss = 0.648526\n",
      "epoch: 0016   loss = 0.553345\n",
      "epoch: 0017   loss = 0.484354\n",
      "epoch: 0018   loss = 0.403943\n",
      "epoch: 0019   loss = 0.348224\n",
      "epoch: 0020   loss = 0.299267\n",
      "epoch: 0021   loss = 0.250771\n",
      "epoch: 0022   loss = 0.220450\n",
      "epoch: 0023   loss = 0.197737\n",
      "epoch: 0024   loss = 0.170108\n",
      "epoch: 0025   loss = 0.129920\n",
      "epoch: 0026   loss = 0.107821\n",
      "epoch: 0027   loss = 0.101173\n",
      "epoch: 0028   loss = 0.084048\n",
      "epoch: 0029   loss = 0.067873\n",
      "epoch: 0030   loss = 0.059228\n",
      "epoch: 0031   loss = 0.050879\n",
      "epoch: 0032   loss = 0.043111\n",
      "epoch: 0033   loss = 0.033273\n",
      "epoch: 0034   loss = 0.028404\n",
      "epoch: 0035   loss = 0.023175\n",
      "epoch: 0036   loss = 0.019150\n",
      "epoch: 0037   loss = 0.016054\n",
      "epoch: 0038   loss = 0.012464\n",
      "epoch: 0039   loss = 0.009897\n",
      "epoch: 0040   loss = 0.007694\n",
      "epoch: 0041   loss = 0.007929\n",
      "epoch: 0042   loss = 0.005337\n",
      "epoch: 0043   loss = 0.005088\n",
      "epoch: 0044   loss = 0.004210\n",
      "epoch: 0045   loss = 0.004050\n",
      "epoch: 0046   loss = 0.002585\n",
      "epoch: 0047   loss = 0.001867\n",
      "epoch: 0048   loss = 0.002039\n",
      "epoch: 0049   loss = 0.001314\n",
      "accuracy=0.93699998\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n",
    "\n",
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, \n",
    "                                                              labels=y))\n",
    "# optimizer function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = mnist.test.images, \n",
    "                          Y_test = mnist.test.labels\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 1.0688 - acc: 0.7467\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.4358 - acc: 0.8813\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.3535 - acc: 0.9003\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.3147 - acc: 0.9103\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2888 - acc: 0.9180\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2689 - acc: 0.9239\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2526 - acc: 0.9279\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2385 - acc: 0.9323\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2255 - acc: 0.9355\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2142 - acc: 0.9391\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.2039 - acc: 0.9419\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1946 - acc: 0.9452\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1857 - acc: 0.9475\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1780 - acc: 0.9498\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1705 - acc: 0.9517\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1635 - acc: 0.9537\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1571 - acc: 0.9551\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1511 - acc: 0.9573\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1456 - acc: 0.9588\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1401 - acc: 0.9604\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1351 - acc: 0.9616\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1306 - acc: 0.9631\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1265 - acc: 0.9646\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1220 - acc: 0.9662\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1183 - acc: 0.9673\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1145 - acc: 0.9683\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1110 - acc: 0.9691\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1077 - acc: 0.9697\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1046 - acc: 0.9711\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.1014 - acc: 0.9719\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0986 - acc: 0.9726\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0960 - acc: 0.9734\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0932 - acc: 0.9743\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0907 - acc: 0.9747\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0883 - acc: 0.9754\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0860 - acc: 0.9761\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0837 - acc: 0.9771\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0816 - acc: 0.9776\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0795 - acc: 0.9781\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0776 - acc: 0.9787\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0756 - acc: 0.9793\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0736 - acc: 0.9804\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0719 - acc: 0.9804\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0704 - acc: 0.9809\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0687 - acc: 0.9813\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0671 - acc: 0.9820\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0656 - acc: 0.9824\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0642 - acc: 0.9829\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0626 - acc: 0.9836\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0613 - acc: 0.9837\n",
      "10000/10000 [==============================] - 0s 40us/step\n",
      "\n",
      "Test loss: 0.08512028525471688\n",
      "Test accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=num_neurons[0], activation='relu', \n",
    "                input_shape=(num_inputs,)))\n",
    "model.add(Dense(units=num_neurons[1], activation='relu'))\n",
    "model.add(Dense(units=num_outputs, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in TFLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 27499  | total loss: \u001b[1m\u001b[32m0.10231\u001b[0m\u001b[0m | time: 2.675s\n",
      "| SGD | epoch: 050 | loss: 0.10231 - acc: 0.9742 -- iter: 54900/55000\n",
      "Training Step: 27500  | total loss: \u001b[1m\u001b[32m0.10065\u001b[0m\u001b[0m | time: 2.680s\n",
      "| SGD | epoch: 050 | loss: 0.10065 - acc: 0.9738 -- iter: 55000/55000\n",
      "--\n",
      "Test accuracy: 0.9653\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Build deep neural network\n",
    "input_layer = tflearn.input_data(shape=[None, num_inputs])\n",
    "dense1 = tflearn.fully_connected(input_layer, num_neurons[0], activation='relu')\n",
    "dense2 = tflearn.fully_connected(dense1, num_neurons[1], activation='relu')\n",
    "softmax = tflearn.fully_connected(dense2, num_outputs, activation='softmax')\n",
    "\n",
    "optimizer = tflearn.SGD(learning_rate=learning_rate)\n",
    "net = tflearn.regression(softmax, optimizer=optimizer, \n",
    "                         metric=tflearn.metrics.Accuracy(), \n",
    "                         loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          n_epoch=n_epochs, batch_size=batch_size, \n",
    "          show_metric=True, run_id='dense_model')\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeries Data - MLP - Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/ubuntu/datasets/ts-data/international-airline-passengers-cleaned.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0e54cd403d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                      \u001b[0;34m'ts-data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      'international-airline-passengers-cleaned.csv'), \n\u001b[0;32m----> 6\u001b[0;31m                         usecols=[1],header=0)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/home/ubuntu/datasets/ts-data/international-airline-passengers-cleaned.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataframe = pd.read_csv(os.path.join(datasetslib.datasets_root, \n",
    "                                     'ts-data', \n",
    "                                     'international-airline-passengers-cleaned.csv'), \n",
    "                        usecols=[1],header=0)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "#scaler = skpp.MinMaxScaler(feature_range=(0, 1))\n",
    "#normalized_dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train,test=dsu.train_test_split(dataset,train_size=0.67)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t-1,t and Y=t+1\n",
    "n_x=2\n",
    "n_y=1\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = dsu.mvts_to_xy(train,test,n_x=n_x,n_y=n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "num_neurons = [8,8]\n",
    "n_epochs = 50\n",
    "batch_size = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_neurons[0], activation='relu', input_shape=(n_x,)))\n",
    "model.add(Dense(num_neurons[1], activation='relu'))\n",
    "model.add(Dense(units=1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('\\nTest mse:', score)\n",
    "print('Test rmse:', math.sqrt(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "Y_train_pred_plot = np.empty_like(dataset)\n",
    "Y_train_pred_plot[:, :] = np.nan\n",
    "Y_train_pred_plot[n_x-1:len(Y_train_pred)+n_x-1, :] = Y_train_pred\n",
    "\n",
    "# shift test predictions for plotting\n",
    "Y_test_pred_plot = np.empty_like(dataset)\n",
    "Y_test_pred_plot[:, :] = np.nan\n",
    "Y_test_pred_plot[len(Y_train_pred)+(n_x*2)-1:len(dataset)-1, :] = Y_test_pred\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(dataset,label='Original Data')\n",
    "plt.plot(Y_train_pred_plot,label='Y_train_pred')\n",
    "plt.plot(Y_test_pred_plot,label='Y_test_pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
