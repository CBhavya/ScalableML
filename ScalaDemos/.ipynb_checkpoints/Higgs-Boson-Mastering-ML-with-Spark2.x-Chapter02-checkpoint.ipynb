{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo code from Mastering Machine Learning with Spark 2.x\n",
    "### Chapter02\n",
    "### https://github.com/PacktPublishing/Mastering-Machine-Learning-with-Spark-2.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import org.apache.spark.mllib.evaluation._\n",
    "import org.apache.spark.mllib.tree._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Tabulizer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.Vector\n",
    "/**\n",
    "  * A simple console based table.\n",
    "  */\n",
    "object Tabulizer {\n",
    "  def table(cells: Seq[Product]): String =\n",
    "    table(cells.map(p => p.productIterator.toList), false)\n",
    "\n",
    "  def table(header: Seq[String], cells: Seq[Product], format: Map[Int, String]): String =\n",
    "    table(Seq(header) ++ cells.map(p => {\n",
    "      p.productIterator.toList.zipWithIndex.map { case (v, i) =>\n",
    "          format.get(i).map(f => String.format(f, v.asInstanceOf[Object])).getOrElse(v)\n",
    "      }\n",
    "    }), true)\n",
    "\n",
    "  def table[A, B](cells: scala.collection.Map[A, B]): String = {\n",
    "    val header = cells.keys.toSeq\n",
    "    val values = header.map(cells(_))\n",
    "    table(Seq(header) ++ Seq(values), true)\n",
    "  }\n",
    "\n",
    "  /*def table(header: Seq[String], cells: Seq[Seq[Any]]): String =\n",
    "    table(Seq(header) ++ cells, header = true)*/\n",
    "\n",
    "  def table(vector: Vector, cols: Int, format: String = \"%.3f\"): String =\n",
    "    table(vector.toArray.map(format.format(_)), cols, None)\n",
    "\n",
    "  def table(list: Seq[Any], cols: Int, header: Option[Seq[String]]): String =\n",
    "    table(tblize(header.map(_ ++ list).getOrElse(list), cols), header.isDefined)\n",
    "\n",
    "  def table(cells: Seq[Seq[Any]], header: Boolean): String = {\n",
    "    val colSizes = cells\n",
    "      .map(_.map(v => if (v != null) v.toString.length else 1))\n",
    "      .reduce((v1, v2) => v1.zip(v2).map { case (v1, v2) => if (v1 > v2) v1 else v2 })\n",
    "    val rowSeparator = colSizes.map(\"-\" * _).mkString(\"+\", \"+\", \"+\")\n",
    "    def valueFormatter(v: Any, size: Int): String =\n",
    "      (\"%\" + size + \"s\").format(if (v != null) v else \"-\")\n",
    "    val rows = cells\n",
    "      .map(row => row.zip(colSizes)\n",
    "        .map { case (v, size) => valueFormatter(v, size) }.mkString(\"|\", \"|\", \"|\"))\n",
    "    if (header)\n",
    "      s\"\"\"\n",
    "         #$rowSeparator\n",
    "         #${rows.head}\n",
    "         #$rowSeparator\n",
    "         #${rows.tail.mkString(\"\\n\")}\n",
    "         #$rowSeparator\n",
    "      \"\"\".stripMargin('#')\n",
    "    else\n",
    "      s\"\"\"\n",
    "         #$rowSeparator\n",
    "         #${rows.mkString(\"\\n\")}\n",
    "         #$rowSeparator\n",
    "      \"\"\".stripMargin('#')\n",
    "  }\n",
    "\n",
    "  def tblize(list: Seq[Product], horizontal: Boolean, cols: Int): Seq[Seq[Any]] = {\n",
    "    val arity = list.head.productArity\n",
    "    tblize(list.flatMap(_.productIterator.toList), cols = arity * cols)\n",
    "  }\n",
    "\n",
    "  def tblize(list: Seq[Any], cols: Int = 4): Seq[Seq[Any]] = {\n",
    "    val nrow = list.length / cols + (if (list.length % cols == 0) 0 else 1)\n",
    "    list.sliding(cols, cols)\n",
    "      .map(s => if (s.length == cols || s.length == list.length) s else s.padTo(cols, null))\n",
    "      .foldLeft(Seq[Seq[Any]]()) { case (a, s) => a ++ Seq(s) }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 100000                                                          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawData = data/higgs100k.csv MapPartitionsRDD[3] at textFile at <console>:40\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data/higgs100k.csv MapPartitionsRDD[3] at textFile at <console>:40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawData = sc.textFile(s\"${sys.env.get(\"DATADIR\").getOrElse(\"data\")}/higgs100k.csv\")\n",
    "println(s\"Number of rows: ${rawData.count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows\n",
      "1.000000000000000000e+00,8.692932128906250000e-01,-6.350818276405334473e-01,2.256902605295181274e-01,3.274700641632080078e-01,-6.899932026863098145e-01,7.542022466659545898e-01,-2.485731393098831177e-01,-1.092063903808593750e+00,0.000000000000000000e+00,1.374992132186889648e+00,-6.536741852760314941e-01,9.303491115570068359e-01,1.107436060905456543e+00,1.138904333114624023e+00,-1.578198313713073730e+00,-1.046985387802124023e+00,0.000000000000000000e+00,6.579295396804809570e-01,-1.045456994324922562e-02,-4.576716944575309753e-02,3.101961374282836914e+00,1.353760004043579102e+00,9.795631170272827148e-01,9.780761599540710449e-01,9.200048446655273438e-01,7.216574549674987793e-01,9.887509346008300781e-01,8.766783475875854492e-01\n",
      "1.000000000000000000e+00,9.075421094894409180e-01,3.291472792625427246e-01,3.594118654727935791e-01,1.497969865798950195e+00,-3.130095303058624268e-01,1.095530629158020020e+00,-5.575249195098876953e-01,-1.588229775428771973e+00,2.173076152801513672e+00,8.125811815261840820e-01,-2.136419266462326050e-01,1.271014571189880371e+00,2.214872121810913086e+00,4.999939501285552979e-01,-1.261431813240051270e+00,7.321561574935913086e-01,0.000000000000000000e+00,3.987008929252624512e-01,-1.138930082321166992e+00,-8.191101951524615288e-04,0.000000000000000000e+00,3.022198975086212158e-01,8.330481648445129395e-01,9.856996536254882812e-01,9.780983924865722656e-01,7.797321677207946777e-01,9.923557639122009277e-01,7.983425855636596680e-01\n"
     ]
    }
   ],
   "source": [
    "println(\"Rows\")\n",
    "println(rawData.take(2).mkString(\"\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = MapPartitionsRDD[4] at map at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[4] at map at <console>:42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = rawData.map(line => line.split(',').map(_.toDouble))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.869293212890625, -0.6350818276405334, 0.22569026052951813, 0.327470064163208, -0.6899932026863098, 0.7542022466659546, -0.24857313930988312, -1.0920639038085938, 0.0, 1.3749921321868896, -0.6536741852760315, 0.9303491115570068, 1.1074360609054565, 1.138904333114624, -1.5781983137130737, -1.046985387802124, 0.0, 0.657929539680481, -0.010454569943249226, -0.0457671694457531, 3.101961374282837, 1.353760004043579, 0.9795631170272827, 0.978076159954071, 0.9200048446655273, 0.7216574549674988, 0.9887509346008301, 0.8766783475875854], [1.0, 0.9075421094894409, 0.3291472792625427, 0.3594118654727936, 1.4979698657989502, -0.3130095303058624, 1.09553062915802, -0.5575249195098877, -1.588229775428772, 2.1730761528015137, 0.8125811815261841, -0.2136419266462326, 1.2710145711898804, 2.214872121810913, 0.4999939501285553, -1.2614318132400513, 0.7321561574935913, 0.0, 0.39870089292526245, -1.138930082321167, -8.191101951524615E-4, 0.0, 0.3022198975086212, 0.8330481648445129, 0.9856996536254883, 0.9780983924865723, 0.7797321677207947, 0.9923557639122009, 0.7983425855636597]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response = MapPartitionsRDD[5] at map at <console>:45\n",
       "features = MapPartitionsRDD[6] at map at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[6] at map at <console>:46"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Split the rows into input(=features) - output(=response) pairs\n",
    "val response: RDD[Int] = data.map(row => row(0).toInt)\n",
    "val features: RDD[Vector] = data.map(line => Vectors.dense(line.slice(1, line.size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.869293212890625,-0.6350818276405334,0.22569026052951813,0.327470064163208,-0.6899932026863098,0.7542022466659546,-0.24857313930988312,-1.0920639038085938,0.0,1.3749921321868896,-0.6536741852760315,0.9303491115570068,1.1074360609054565,1.138904333114624,-1.5781983137130737,-1.046985387802124,0.0,0.657929539680481,-0.010454569943249226,-0.0457671694457531,3.101961374282837,1.353760004043579,0.9795631170272827,0.978076159954071,0.9200048446655273,0.7216574549674988,0.9887509346008301,0.8766783475875854], [0.9075421094894409,0.3291472792625427,0.3594118654727936,1.4979698657989502,-0.3130095303058624,1.09553062915802,-0.5575249195098877,-1.588229775428772,2.1730761528015137,0.8125811815261841,-0.2136419266462326,1.2710145711898804,2.214872121810913,0.4999939501285553,-1.2614318132400513,0.7321561574935913,0.0,0.39870089292526245,-1.138930082321167,-8.191101951524615E-4,0.0,0.3022198975086212,0.8330481648445129,0.9856996536254883,0.9780983924865723,0.7797321677207947,0.9923557639122009,0.7983425855636597]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================>                                       (1 + 2) / 3]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featuresMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@18bc80ce\n",
       "featuresSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@7bf2d16e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@7bf2d16e"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featuresMatrix = new RowMatrix(features)\n",
    "val featuresSummary = featuresMatrix.computeColumnSummaryStatistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higgs Features Mean Values = \n",
      "+-----+------+------+------+------+-----+------+-----+\n",
      "|0.990|-0.004|-0.002| 0.995|-0.008|0.987|-0.003|0.000|\n",
      "|0.998| 0.991|-0.001| 0.004| 1.004|0.993| 0.002|0.001|\n",
      "|1.006| 0.986|-0.008|-0.004| 0.993|1.033| 1.023|1.050|\n",
      "|1.010| 0.973| 1.032| 0.959|     -|    -|     -|    -|\n",
      "+-----+------+------+------+------+-----+------+-----+\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "//Print mean of columns\n",
    "println(s\"Higgs Features Mean Values = ${Tabulizer.table(featuresSummary.mean, 8)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higgs Features Variance Values = \n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|0.316|1.010|1.012|0.354|1.014|0.224|1.017|1.017|\n",
      "|1.056|0.248|1.010|1.014|1.101|0.238|1.017|1.011|\n",
      "|1.431|0.255|1.018|1.014|1.951|0.426|0.138|0.027|\n",
      "|0.159|0.274|0.132|0.098|    -|    -|    -|    -|\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "//Print the variance of columns\n",
    "println(s\"Higgs Features Variance Values = ${Tabulizer.table(featuresSummary.variance, 8)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero values count per column: \n",
      "+------+------+------+------+------+------+------+------+\n",
      "|100000|100000|100000|100000|100000|100000|100000|100000|\n",
      "| 50907|100000|100000|100000| 50023|100000|100000|100000|\n",
      "| 43176|100000|100000|100000| 34973|100000|100000|100000|\n",
      "|100000|100000|100000|100000|     -|     -|     -|     -|\n",
      "+------+------+------+------+------+------+------+------+\n",
      "      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nonZeros = [100000.0,100000.0,100000.0,100000.0,100000.0,100000.0,100000.0,100000.0,50907.0,100000.0,100000.0,100000.0,50023.0,100000.0,100000.0,100000.0,43176.0,100000.0,100000.0,100000.0,34973.0,100000.0,100000.0,100000.0,100000.0,100000.0,100000.0,100000.0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[100000.0,100000.0,100000.0,100000.0,100000.0,100000.0,100000.0,100000.0,50907.0,100000.0,100000.0,100000.0,50023.0,100000.0,100000.0,100000.0,43176.0,100000.0,100000.0,100000.0,34973.0,100000.0,100000.0,100000.0,100000.0,100000.0,100000.0,100000.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculate how many nonzeros we have in each column\n",
    "val nonZeros = featuresSummary.numNonzeros\n",
    "println(s\"Non-zero values count per column: ${Tabulizer.table(nonZeros, cols = 8, format = \"%.0f\")}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numRows = 100000\n",
       "numCols = 28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numRows = featuresMatrix.numRows\n",
    "val numCols = featuresMatrix.numCols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colsWithZeros = Array((50907.0,8), (50023.0,12), (43176.0,16), (34973.0,20))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(50907.0,8), (50023.0,12), (43176.0,16), (34973.0,20)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colsWithZeros = nonZeros\n",
    "  .toArray\n",
    "  .zipWithIndex\n",
    "  .filter { case (rows, idx) => rows != numRows }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with zeros:\n",
      "\n",
      "+-------+------+\n",
      "| #zeros|column|\n",
      "+-------+------+\n",
      "|50907.0|     8|\n",
      "|50023.0|    12|\n",
      "|43176.0|    16|\n",
      "|34973.0|    20|\n",
      "+-------+------+\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "println(s\"Columns with zeros:\\n${Tabulizer.table(Seq(\"#zeros\", \"column\"), colsWithZeros, Map.empty[Int, String])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sparsity: 0.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparsity = 0.9210996428571429\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9210996428571429"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparsity = nonZeros.toArray.sum / (numRows * numCols)\n",
    "println(f\"Data sparsity: ${sparsity}%.2f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response values: 0, 1                                                           \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "responseValues = Array(0, 1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val responseValues = response.distinct.collect\n",
    "println(s\"Response values: ${responseValues.mkString(\", \")}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response distribution:                                                          \n",
      "\n",
      "+-----+-----+\n",
      "|    0|    1|\n",
      "+-----+-----+\n",
      "|47166|52834|\n",
      "+-----+-----+\n",
      "      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "responseDistribution = Map(0 -> 47166, 1 -> 52834)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(0 -> 47166, 1 -> 52834)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val responseDistribution = response.map(v => (v,1)).countByKey\n",
    "println(s\"Response distribution:\\n${Tabulizer.table(responseDistribution)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "higgs = higgs MapPartitionsRDD[15] at map at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "higgs MapPartitionsRDD[15] at map at <console>:47"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val higgs = response.zip(features).map { case (response, features) => LabeledPoint(response, features) }\n",
    "higgs.setName(\"higgs\").cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainTestSplits = Array(MapPartitionsRDD[16] at randomSplit at <console>:49, MapPartitionsRDD[17] at randomSplit at <console>:49)\n",
       "trainingData = MapPartitionsRDD[16] at randomSplit at <console>:49\n",
       "testData = MapPartitionsRDD[17] at randomSplit at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[17] at randomSplit at <console>:49"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainTestSplits = higgs.randomSplit(Array(0.8, 0.2))\n",
    "val (trainingData, testData) = (trainTestSplits(0), trainTestSplits(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  === Tree Model ===  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtNumClasses = 2\n",
       "dtCategoricalFeaturesInfo = Map()\n",
       "dtImpurity = gini\n",
       "dtMaxDepth = 5\n",
       "dtMaxBins = 10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dtNumClasses = 2\n",
    "val dtCategoricalFeaturesInfo = Map[Int, Int]()\n",
    "val dtImpurity = \"gini\"\n",
    "val dtMaxDepth = 5\n",
    "val dtMaxBins = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 3) / 3]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtreeModel = DecisionTreeModel classifier of depth 5 with 63 nodes\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeModel classifier of depth 5 with 63 nodes"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dtreeModel = DecisionTree.trainClassifier(trainingData,\n",
    "                                              dtNumClasses,\n",
    "                                              dtCategoricalFeaturesInfo,\n",
    "                                              dtImpurity,\n",
    "                                              dtMaxDepth,\n",
    "                                              dtMaxBins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model:\n",
      "DecisionTreeModel classifier of depth 5 with 63 nodes\n",
      "  If (feature 25 <= 1.0560009479522705)\n",
      "   If (feature 25 <= 0.6134785413742065)\n",
      "    If (feature 27 <= 0.8689159750938416)\n",
      "     If (feature 5 <= 0.8908801674842834)\n",
      "      If (feature 22 <= 0.7452906966209412)\n",
      "       Predict: 1.0\n",
      "      Else (feature 22 > 0.7452906966209412)\n",
      "       Predict: 0.0\n",
      "     Else (feature 5 > 0.8908801674842834)\n",
      "      If (feature 27 <= 0.7907640337944031)\n",
      "       Predict: 1.0\n",
      "      Else (feature 27 > 0.7907640337944031)\n",
      "       Predict: 1.0\n",
      "    Else (feature 27 > 0.8689159750938416)\n",
      "     If (feature 22 <= 0.9895526170730591)\n",
      "      If (feature 24 <= 1.0830920934677124)\n",
      "       Predict: 0.0\n",
      "      Else (feature 24 > 1.0830920934677124)\n",
      "       Predict: 0.0\n",
      "     Else (feature 22 > 0.9895526170730591)\n",
      "      If (feature 5 <= 1.5511850118637085)\n",
      "       Predict: 0.0\n",
      "      Else (feature 5 > 1.5511850118637085)\n",
      "       Predict: 1.0\n",
      "   Else (feature 25 > 0.6134785413742065)\n",
      "    If (feature 26 <= 0.7856372594833374)\n",
      "     If (feature 5 <= 0.8072428107261658)\n",
      "      If (feature 22 <= 1.1286323070526123)\n",
      "       Predict: 0.0\n",
      "      Else (feature 22 > 1.1286323070526123)\n",
      "       Predict: 1.0\n",
      "     Else (feature 5 > 0.8072428107261658)\n",
      "      If (feature 27 <= 0.7907640337944031)\n",
      "       Predict: 1.0\n",
      "      Else (feature 27 > 0.7907640337944031)\n",
      "       Predict: 0.0\n",
      "    Else (feature 26 > 0.7856372594833374)\n",
      "     If (feature 27 <= 0.9257105588912964)\n",
      "      If (feature 26 <= 0.8491965532302856)\n",
      "       Predict: 1.0\n",
      "      Else (feature 26 > 0.8491965532302856)\n",
      "       Predict: 1.0\n",
      "     Else (feature 27 > 0.9257105588912964)\n",
      "      If (feature 22 <= 1.0439356565475464)\n",
      "       Predict: 1.0\n",
      "      Else (feature 22 > 1.0439356565475464)\n",
      "       Predict: 1.0\n",
      "  Else (feature 25 > 1.0560009479522705)\n",
      "   If (feature 25 <= 1.6191602945327759)\n",
      "    If (feature 22 <= 1.0439356565475464)\n",
      "     If (feature 24 <= 1.0830920934677124)\n",
      "      If (feature 27 <= 0.9257105588912964)\n",
      "       Predict: 0.0\n",
      "      Else (feature 27 > 0.9257105588912964)\n",
      "       Predict: 0.0\n",
      "     Else (feature 24 > 1.0830920934677124)\n",
      "      If (feature 27 <= 1.1188242435455322)\n",
      "       Predict: 1.0\n",
      "      Else (feature 27 > 1.1188242435455322)\n",
      "       Predict: 0.0\n",
      "    Else (feature 22 > 1.0439356565475464)\n",
      "     If (feature 3 <= 1.7399213314056396)\n",
      "      If (feature 5 <= 1.101759910583496)\n",
      "       Predict: 1.0\n",
      "      Else (feature 5 > 1.101759910583496)\n",
      "       Predict: 1.0\n",
      "     Else (feature 3 > 1.7399213314056396)\n",
      "      If (feature 27 <= 1.1188242435455322)\n",
      "       Predict: 0.0\n",
      "      Else (feature 27 > 1.1188242435455322)\n",
      "       Predict: 0.0\n",
      "   Else (feature 25 > 1.6191602945327759)\n",
      "    If (feature 22 <= 0.9895526170730591)\n",
      "     If (feature 27 <= 1.0040874481201172)\n",
      "      If (feature 5 <= 1.5511850118637085)\n",
      "       Predict: 0.0\n",
      "      Else (feature 5 > 1.5511850118637085)\n",
      "       Predict: 1.0\n",
      "     Else (feature 27 > 1.0040874481201172)\n",
      "      If (feature 24 <= 1.0830920934677124)\n",
      "       Predict: 0.0\n",
      "      Else (feature 24 > 1.0830920934677124)\n",
      "       Predict: 0.0\n",
      "    Else (feature 22 > 0.9895526170730591)\n",
      "     If (feature 0 <= 1.3412736654281616)\n",
      "      If (feature 3 <= 1.4083144664764404)\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 > 1.4083144664764404)\n",
      "       Predict: 0.0\n",
      "     Else (feature 0 > 1.3412736654281616)\n",
      "      If (feature 24 <= 1.2264255285263062)\n",
      "       Predict: 0.0\n",
      "      Else (feature 24 > 1.2264255285263062)\n",
      "       Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(s\"Decision Tree Model:\\n${dtreeModel.toDebugString}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treeLabelAndPreds = MapPartitionsRDD[42] at map at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[42] at map at <console>:49"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val treeLabelAndPreds = testData.map { point =>\n",
    "  val prediction = dtreeModel.predict(point.features)\n",
    "  (point.label.toInt, prediction.toInt)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Model: Test Error = 0.333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "treeTestErr = 0.33283648829929946\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.33283648829929946"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val treeTestErr = treeLabelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\n",
    "println(f\"Tree Model: Test Error = ${treeTestErr}%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cm = Array((0,(5430,4083)), (1,(2616,7998)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0,(5430,4083)), (1,(2616,7998))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cm = treeLabelAndPreds.combineByKey(\n",
    "  createCombiner = (label: Int) => if (label == 0) (1,0) else (0,1),\n",
    "  mergeValue = (v: (Int,Int), label: Int) => if (label == 0) (v._1 +1, v._2) else (v._1, v._2 + 1),\n",
    "  mergeCombiners = (v1: (Int,Int), v2: (Int,Int)) => (v1._1 + v2._1, v1._2 + v2._2)).collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "       0     1         Err\n",
      "0   5430  4083  9513 0.4292\n",
      "1   2616  7998 10614 0.2465\n",
      "    8046 12081 20127 0.3328\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tn = 5430\n",
       "tp = 7998\n",
       "fn = 2616\n",
       "fp = 4083\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4083"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (tn, tp, fn, fp) = (cm(0)._2._1, cm(1)._2._2, cm(1)._2._1, cm(0)._2._2)\n",
    "println(f\"\"\"Confusion Matrix\n",
    "            |   ${0}%5d ${1}%5d  ${\"Err\"}%10s\n",
    "            |0  ${tn}%5d ${fp}%5d ${tn+fp}%5d ${fp.toDouble/(tn+fp)}%5.4f\n",
    "            |1  ${fn}%5d ${tp}%5d ${fn+tp}%5d ${fn.toDouble/(fn+tp)}%5.4f\n",
    "            |   ${tn+fn}%5d ${fp+tp}%5d ${tn+fp+fn+tp}%5d ${(fp+fn).toDouble/(tn+fp+fn+tp)}%5.4f\n",
    "            |\"\"\".stripMargin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias Predictor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type Predictor = {\n",
    "  def predict(features: Vector): Double\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n",
       "computeMetrics: (model: Predictor, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def computeMetrics(model: Predictor, data: RDD[LabeledPoint]): BinaryClassificationMetrics = {\n",
    "  val predAndLabels = data.map(newData => (model.predict(newData.features), newData.label))\n",
    "  new BinaryClassificationMetrics(predAndLabels)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Model: AUC on Test Data = 0.662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "treeMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@1479d9e7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@1479d9e7"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val treeMetrics = computeMetrics(dtreeModel, testData)\n",
    "println(f\"Tree Model: AUC on Test Data = ${treeMetrics.areaUnderROC()}%.3f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === Random Forest Model ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses = 2\n",
       "categoricalFeaturesInfo = Map()\n",
       "numTrees = 10\n",
       "featureSubsetStrategy = auto\n",
       "impurity = gini\n",
       "maxDepth = 5\n",
       "maxBins = 10\n",
       "seed = 42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val numTrees = 10\n",
    "val featureSubsetStrategy = \"auto\"\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 10\n",
    "val seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfModel = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TreeEnsembleModel classifier with 10 trees\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfModel = RandomForest.trainClassifier(trainingData,\n",
    "                                           numClasses,\n",
    "                                           categoricalFeaturesInfo,\n",
    "                                           numTrees,\n",
    "                                           featureSubsetStrategy,\n",
    "                                           impurity,\n",
    "                                           maxDepth,\n",
    "                                           maxBins,\n",
    "                                           seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n",
       "computeError: (model: Predictor, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def computeError(model: Predictor, data: RDD[LabeledPoint]): Double = {\n",
    "  val labelAndPreds = data.map { point =>\n",
    "    val prediction = model.predict(point.features)\n",
    "    (point.label, prediction)\n",
    "                               }\n",
    "  labelAndPreds.filter(r => r._1 != r._2).count.toDouble/data.count\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Model: Test Error = 0.326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rfTestErr = 0.3264768718636657\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3264768718636657"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfTestErr = computeError(rfModel, testData)\n",
    "println(f\"RF Model: Test Error = ${rfTestErr}%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Model: AUC on Test Data = 0.668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rfMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@20f47655\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@20f47655"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfMetrics = computeMetrics(rfModel, testData)\n",
    "println(f\"RF Model: AUC on Test Data = ${rfMetrics.areaUnderROC}%.3f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below cell might take some time to run + we might also need large enough memory depending on the grid search parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfGrid = Array(((10,entropy,10,12),0.6917657292864599,0.30426789884235106), ((10,entropy,10,14),0.693615359708,0.30223083420281216), ((10,gini,10,12),0.6909380707023332,0.3052615889104188), ((10,gini,10,14),0.6888254488799563,0.3068514930193273), ((11,entropy,10,12),0.6978558156441422,0.30054156108709695), ((11,entropy,10,14),0.6893462173122175,0.307199284543151), ((11,gini,10,12),0.6993283476236767,0.2994485020122224), ((11,gini,10,14),0.6943655009713582,0.30396979182193073))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[((10,entropy,10,12),0.6917657292864599,0.30426789884235106), ((10,entropy,10,14),0.693615359708,0.30223083420281216), ((10,gini,10,12),0.6909380707023332,0.3052615889104188), ((10,gini,10,14),0.6888254488799563,0.3068514930193273), ((11,entropy,10,12),0.6978558156441422,0.30054156108709695), ((11,entropy,10,14),0.6893462173122175,0.307199284543151), ((11,gini,10,12),0.6993283476236767,0.2994485020122224), ((11,gini,10,14),0.6943655009713582,0.30396979182193073)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfGrid =\n",
    "for (\n",
    "  gridNumTrees <- Array(10, 11);\n",
    "  gridImpurity <- Array(\"entropy\", \"gini\");\n",
    "  gridDepth <- Array(10);\n",
    "  gridBins <- Array(12,14)) yield {\n",
    "  val gridModel = RandomForest.trainClassifier(trainingData, 2, Map[Int, Int](), gridNumTrees, \"auto\", gridImpurity, gridDepth, gridBins)\n",
    "  val gridAUC = computeMetrics(gridModel, testData).areaUnderROC\n",
    "  val gridErr = computeError(gridModel, testData)\n",
    "  ((gridNumTrees, gridImpurity, gridDepth, gridBins), gridAUC, gridErr)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Model: Grid results:\n",
      "\n",
      "+----------------------------+-----+-----+\n",
      "|trees, impurity, depth, bins|  AUC|error|\n",
      "+----------------------------+-----+-----+\n",
      "|          (10,entropy,10,12)|0.692|0.304|\n",
      "|          (10,entropy,10,14)|0.694|0.302|\n",
      "|             (10,gini,10,12)|0.691|0.305|\n",
      "|             (10,gini,10,14)|0.689|0.307|\n",
      "|          (11,entropy,10,12)|0.698|0.301|\n",
      "|          (11,entropy,10,14)|0.689|0.307|\n",
      "|             (11,gini,10,12)|0.699|0.299|\n",
      "|             (11,gini,10,14)|0.694|0.304|\n",
      "+----------------------------+-----+-----+\n",
      "      \n",
      "   \n"
     ]
    }
   ],
   "source": [
    "println(\n",
    "  s\"\"\"RF Model: Grid results:\n",
    "     ~${Tabulizer.table(Seq(\"trees, impurity, depth, bins\", \"AUC\", \"error\"), rfGrid, format = Map(1 -> \"%.3f\", 2 -> \"%.3f\"))}\n",
    "   \"\"\".stripMargin('~'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Model: Parameters (11,gini,10,12) producing max AUC = 0.699 (error = 0.299)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rfParamsMaxAUC = ((11,gini,10,12),0.6993283476236767,0.2994485020122224)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((11,gini,10,12),0.6993283476236767,0.2994485020122224)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfParamsMaxAUC = rfGrid.maxBy(g => g._2)\n",
    "println(f\"RF Model: Parameters ${rfParamsMaxAUC._1}%s producing max AUC = ${rfParamsMaxAUC._2}%.3f (error = ${rfParamsMaxAUC._3}%.3f)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === Gradient Boosted Trees Model ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.GradientBoostedTrees\n",
    "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
    "import org.apache.spark.mllib.tree.configuration.Algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gbmStrategy = BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@4db66399,org.apache.spark.mllib.tree.loss.LogLoss$@dc2fc01,10,0.1,0.001)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@4db66399,org.apache.spark.mllib.tree.loss.LogLoss$@dc2fc01,10,0.1,0.001)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gbmStrategy = BoostingStrategy.defaultParams(Algo.Classification)\n",
    "gbmStrategy.setNumIterations(10)\n",
    "gbmStrategy.setLearningRate(0.1)\n",
    "gbmStrategy.treeStrategy.setNumClasses(2)\n",
    "gbmStrategy.treeStrategy.setMaxDepth(10)\n",
    "gbmStrategy.treeStrategy.setCategoricalFeaturesInfo(java.util.Collections.emptyMap[Integer, Integer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gbmModel = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TreeEnsembleModel classifier with 10 trees\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gbmModel = GradientBoostedTrees.train(trainingData, gbmStrategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Model: Test Error = 0.304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gbmTestErr = 0.30436726784915785\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.30436726784915785"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gbmTestErr = computeError(gbmModel, testData)\n",
    "println(f\"GBM Model: Test Error = ${gbmTestErr}%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Model: AUC on Test Data = 0.662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gbmMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4b3c5fe9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4b3c5fe9"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gbmMetrics = computeMetrics(dtreeModel, testData)\n",
    "println(f\"GBM Model: AUC on Test Data = ${gbmMetrics.areaUnderROC()}%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gbmGrid = Array(((5,2,0.1),0.6392499431173206,0.3590699060962886), ((5,2,0.01),0.6334560953363809,0.3665722661102002), ((5,3,0.1),0.6676326323141039,0.33114721518358425), ((5,3,0.01),0.6541108365173669,0.3441148705718686), ((10,2,0.1),0.646501259143939,0.3509216475381329), ((10,2,0.01),0.6334560953363809,0.3665722661102002), ((10,3,0.1),0.6754064846076272,0.3227008496050082), ((10,3,0.01),0.6618918641397387,0.33626471903413324))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[((5,2,0.1),0.6392499431173206,0.3590699060962886), ((5,2,0.01),0.6334560953363809,0.3665722661102002), ((5,3,0.1),0.6676326323141039,0.33114721518358425), ((5,3,0.01),0.6541108365173669,0.3441148705718686), ((10,2,0.1),0.646501259143939,0.3509216475381329), ((10,2,0.01),0.6334560953363809,0.3665722661102002), ((10,3,0.1),0.6754064846076272,0.3227008496050082), ((10,3,0.01),0.6618918641397387,0.33626471903413324)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gbmGrid =\n",
    "for (\n",
    "  gridNumIterations <- Array(5, 10);\n",
    "  gridDepth <- Array(2, 3);\n",
    "  gridLearningRate <- Array(0.1, 0.01))\n",
    "  yield {\n",
    "    gbmStrategy.setNumIterations(gridNumIterations)\n",
    "    gbmStrategy.treeStrategy.setMaxDepth(gridDepth)\n",
    "    gbmStrategy.setLearningRate(gridLearningRate)\n",
    "\n",
    "    val gridModel = GradientBoostedTrees.train(trainingData, gbmStrategy)\n",
    "    val gridAUC = computeMetrics(gridModel, testData).areaUnderROC\n",
    "    val gridErr = computeError(gridModel, testData)\n",
    "    ((gridNumIterations, gridDepth, gridLearningRate), gridAUC, gridErr)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Model: Grid results:\n",
      "\n",
      "+-------------------------------+-----+-----+\n",
      "|iterations, depth, learningRate|  AUC|error|\n",
      "+-------------------------------+-----+-----+\n",
      "|                     (10,3,0.1)|0.675|0.323|\n",
      "|                      (5,3,0.1)|0.668|0.331|\n",
      "|                    (10,3,0.01)|0.662|0.336|\n",
      "|                     (5,3,0.01)|0.654|0.344|\n",
      "|                     (10,2,0.1)|0.647|0.351|\n",
      "|                      (5,2,0.1)|0.639|0.359|\n",
      "|                     (5,2,0.01)|0.633|0.367|\n",
      "|                    (10,2,0.01)|0.633|0.367|\n",
      "+-------------------------------+-----+-----+\n",
      "      \n",
      "   \n"
     ]
    }
   ],
   "source": [
    "println(\n",
    "  s\"\"\"GBM Model: Grid results:\n",
    "     ~${Tabulizer.table(Seq(\"iterations, depth, learningRate\", \"AUC\", \"error\"), gbmGrid.sortBy(-_._2).take(10), format = Map(1 -> \"%.3f\", 2 -> \"%.3f\"))}\n",
    "   \"\"\".stripMargin('~'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Model: Parameters (10,3,0.1) producing max AUC = 0.675 (error = 0.323)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gbmParamsMaxAUC = ((10,3,0.1),0.6754064846076272,0.3227008496050082)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((10,3,0.1),0.6754064846076272,0.3227008496050082)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gbmParamsMaxAUC = gbmGrid.maxBy(g => g._2)\n",
    "println(f\"GBM Model: Parameters ${gbmParamsMaxAUC._1}%s producing max AUC = ${gbmParamsMaxAUC._2}%.3f (error = ${gbmParamsMaxAUC._3}%.3f)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
